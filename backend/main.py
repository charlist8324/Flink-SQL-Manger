"""
Flink SQL ç®¡ç†ç³»ç»Ÿ - åç«¯ä¸»ç¨‹åº
"""
import logging
import time
from pathlib import Path
import uuid
from fastapi import FastAPI, HTTPException, UploadFile, File, Query
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse
from typing import List, Optional
from contextlib import asynccontextmanager

# é…ç½®æ—¥å¿—
LOG_DIR = Path(__file__).resolve().parent.parent / "logs"
LOG_DIR.mkdir(exist_ok=True)
LOG_FILE = LOG_DIR / "run.log"

logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s [%(levelname)s] %(name)s - %(message)s',
    handlers=[
        logging.FileHandler(LOG_FILE, encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)
logger.info(f"=== Flink Manager å¯åŠ¨ ===")
logger.info(f"æ—¥å¿—æ–‡ä»¶: {LOG_FILE}")

from .config import FLINK_REST_URL, SQL_FILES_DIR, BASE_DIR
from .database import db_settings, get_database_url
from .db_operations import (
    db_manager, save_job, update_job_state, get_job, get_all_jobs,
    save_savepoint, get_latest_savepoint, get_savepoints, log_operation,
    add_datasource, get_datasources, update_datasource, delete_datasource, test_datasource_connection
)
from .schemas import (
    SqlJobSubmitRequest, JarJobSubmitRequest, JobSubmitResponse,
    ClusterStatus, JobInfo, JobDetail, SavepointRequest, ResumeJobRequest,
    DataSourceCreateRequest, DataSourceUpdateRequest, DataSourceResponse,
    DataSourceBase, TableCreateRequest
)
from .flink_client import flink_client, FlinkClient
from sqlalchemy import create_engine, inspect, text
from pydantic import BaseModel
from urllib.parse import quote_plus

class DatabaseConnectionRequest(BaseModel):
    host: str
    port: int
    username: str
    password: str
    database: str






# ============ åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç† ============

@asynccontextmanager
async def lifespan(app: FastAPI):
    """åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†"""
    # å¯åŠ¨æ—¶åˆå§‹åŒ–æ•°æ®åº“
    logger.info("æ­£åœ¨åˆå§‹åŒ–æ•°æ®åº“è¿æ¥...")
    try:
        db_manager.initialize()
        
        # è‡ªåŠ¨ä¿®å¤æ•°æ®åº“ Schema
        session = db_manager.get_session()
        try:
            logger.info("æ­£åœ¨æ£€æŸ¥æ•°æ®åº“ Schema...")
            # 1. æ£€æŸ¥ flink_jobs è¡¨
            result = session.execute(text("SHOW COLUMNS FROM flink_jobs LIKE 'savepoint_timestamp'"))
            if not result.fetchone():
                logger.info("âš ï¸ flink_jobs è¡¨ç¼ºå°‘ savepoint_timestamp å­—æ®µï¼Œæ­£åœ¨æ·»åŠ ...")
                session.execute(text("ALTER TABLE flink_jobs ADD COLUMN savepoint_timestamp BIGINT COMMENT 'Savepointæ—¶é—´æˆ³(æ¯«ç§’)'"))
                session.commit()
                logger.info("âœ… savepoint_timestamp å­—æ®µå·²æ·»åŠ ")
            
            # 2. æ£€æŸ¥ flink_savepoints è¡¨
            result = session.execute(text("SHOW COLUMNS FROM flink_savepoints LIKE 'timestamp'"))
            if not result.fetchone():
                logger.info("âš ï¸ flink_savepoints è¡¨ç¼ºå°‘ timestamp å­—æ®µï¼Œæ­£åœ¨æ·»åŠ ...")
                session.execute(text("ALTER TABLE flink_savepoints ADD COLUMN timestamp BIGINT COMMENT 'æ—¶é—´æˆ³(æ¯«ç§’)'"))
                session.commit()
                logger.info("âœ… timestamp å­—æ®µå·²æ·»åŠ ")
                
        except Exception as e:
            logger.error(f"âŒ Schema æ£€æŸ¥/ä¿®å¤å¤±è´¥: {e}")
            # ä¸é˜»æ–­å¯åŠ¨ï¼Œå› ä¸ºå¯èƒ½åªæ˜¯æƒé™é—®é¢˜æˆ–å·²å­˜åœ¨
        finally:
            session.close()
            
        logger.info("âœ… æ•°æ®åº“åˆå§‹åŒ–æˆåŠŸ")
    except Exception as e:
        logger.error(f"âŒ æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥: {e}")
        raise
    
    yield
    
    # å…³é—­æ—¶æ¸…ç†èµ„æº
    logger.info("æ­£åœ¨å…³é—­æ•°æ®åº“è¿æ¥...")
    db_manager.close()
    logger.info("æ•°æ®åº“è¿æ¥å·²å…³é—­")


# ============ FastAPI åº”ç”¨ ============

app = FastAPI(
    title="Flink SQL Manager",
    description="Flink SQLä½œä¸šç®¡ç†ç³»ç»Ÿ",
    version="1.0.0",
    lifespan=lifespan
)

# CORSä¸­é—´ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.post("/api/metadata/tables", tags=["ç³»ç»Ÿ"])
async def get_database_tables(request: DatabaseConnectionRequest):
    """è¿æ¥æ•°æ®åº“å¹¶è·å–è¡¨ååˆ—è¡¨"""
    try:
        from urllib.parse import quote_plus
        # æ„å»º MySQL è¿æ¥å­—ç¬¦ä¸²
        # æ ¼å¼: mysql+pymysql://user:password@host:port/database
        encoded_user = quote_plus(request.username)
        encoded_password = quote_plus(request.password)
        db_url = f"mysql+pymysql://{encoded_user}:{encoded_password}@{request.host}:{request.port}/{request.database}"
        
        # åˆ›å»ºå¼•æ“
        engine = create_engine(db_url)
        
        # ä½¿ç”¨ inspect è·å–è¡¨å
        inspector = inspect(engine)
        tables = inspector.get_table_names()
        
        return {"tables": tables}
    except Exception as e:
        logger.error(f"è·å–è¡¨åå¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"è¿æ¥æ•°æ®åº“å¤±è´¥: {str(e)}")


@app.post("/api/metadata/tables/create", tags=["ç³»ç»Ÿ"])
async def create_table(request: TableCreateRequest):
    """åˆ›å»ºç‰©ç†è¡¨"""
    try:
        from urllib.parse import quote_plus
        
        host = request.host
        port = request.port
        username = request.username
        password = request.password
        database = request.database
        
        # å¦‚æœæä¾›äº† datasource_idï¼Œä»æ•°æ®åº“è·å–è¿æ¥ä¿¡æ¯
        if request.datasource_id:
            ds = get_datasources(ds_id=request.datasource_id)
            if not ds:
                raise HTTPException(status_code=404, detail="æ•°æ®æºä¸å­˜åœ¨")
            host = ds.host
            port = ds.port
            username = ds.username
            password = ds.password
            database = ds.database
            
            encoded_user = quote_plus(username)
            encoded_password = quote_plus(password) if password else ""
            db_url = f"mysql+pymysql://{encoded_user}:{encoded_password}@{host}:{port}/{database}"
            
        elif request.url:
            # å°è¯•ä» URL è§£ææˆ–è½¬æ¢
            # å‡è®¾æ˜¯ jdbc:mysql://...
            db_url = request.url
            if db_url.startswith("jdbc:mysql://"):
                db_url = db_url.replace("jdbc:mysql://", "mysql+pymysql://")
            elif db_url.startswith("jdbc:clickhouse://"):
                 # ClickHouse æ”¯æŒæš‚æœªå®Œå…¨å®ç°
                 pass
            
            # å¦‚æœæä¾›äº†ç”¨æˆ·åå¯†ç ï¼Œä½† URL ä¸­æ²¡æœ‰ï¼ˆç®€å•åˆ¤æ–­ï¼‰ï¼Œè¿™æ¯”è¾ƒå¤æ‚
            # è¿™é‡Œç®€åŒ–å¤„ç†ï¼šå¦‚æœæä¾›äº† urlï¼Œå°±ç›´æ¥ç”¨ï¼Œå¦‚æœ URL é‡Œæ²¡å¯†ç é‚£å°±ä¼šå¤±è´¥
            # æ›´å¥½çš„æ–¹å¼æ˜¯è§£æ URLï¼Œä½†è¿™é‡Œä¸ºäº†å…¼å®¹æ€§ï¼Œæˆ‘ä»¬å°½é‡ä¾èµ– datasource_id
            
            # å¦‚æœ URL è½¬æ¢åè¿˜æ˜¯ä¸ç¬¦åˆ SQLAlchemy æ ¼å¼ï¼Œå¯èƒ½éœ€è¦è¿›ä¸€æ­¥å¤„ç†
            # æš‚æ—¶å‡è®¾ç”¨æˆ·è¾“å…¥çš„ URL æˆ–è€…å‰ç«¯ä¼ æ¥çš„ URL æ˜¯æ ‡å‡†çš„ JDBC MySQL
            
            # æ³¨å…¥ç”¨æˆ·åå¯†ç ï¼ˆå¦‚æœ URL æ²¡åŒ…å«ï¼‰
            if username and password and '@' not in db_url:
                # è¿™æ˜¯ä¸€ä¸ªéå¸¸ç²—ç³™çš„æ³¨å…¥ï¼Œä»…ä½œ fallback
                # mysql+pymysql://host:port/db -> mysql+pymysql://user:pass@host:port/db
                prefix = "mysql+pymysql://"
                if db_url.startswith(prefix):
                    rest = db_url[len(prefix):]
                    encoded_user = quote_plus(username)
                    encoded_password = quote_plus(password)
                    db_url = f"{prefix}{encoded_user}:{encoded_password}@{rest}"

        else:
             if not host or not port or not username or not database:
                  raise HTTPException(status_code=400, detail="ç¼ºå°‘æ•°æ®åº“è¿æ¥ä¿¡æ¯")
             
             # æ„å»ºè¿æ¥å­—ç¬¦ä¸²
             encoded_user = quote_plus(username)
             encoded_password = quote_plus(password) if password else ""
             db_url = f"mysql+pymysql://{encoded_user}:{encoded_password}@{host}:{port}/{database}"
        
        engine = create_engine(db_url)
        
        # æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
        inspector = inspect(engine)
        if request.table_name in inspector.get_table_names():
            return {"status": "skipped", "message": f"è¡¨ {request.table_name} å·²å­˜åœ¨"}
            
        # ç”Ÿæˆ CREATE TABLE SQL
        columns_sql = []
        primary_keys = []
        
        for field in request.fields:
            # ç±»å‹æ˜ å°„
            sql_type = field.type.upper()
            if sql_type == 'STRING':
                length = field.precision if field.precision else '255'
                sql_type = f"VARCHAR({length})"
            elif sql_type in ['INT', 'INTEGER']:
                sql_type = "INT"
            elif sql_type == 'BIGINT':
                sql_type = "BIGINT"
            elif sql_type == 'BOOLEAN':
                sql_type = "BOOLEAN"
            elif sql_type in ['TIMESTAMP', 'TIMESTAMP_LTZ']:
                sql_type = "DATETIME"
            elif sql_type == 'DATE':
                sql_type = "DATE"
            elif sql_type == 'DECIMAL':
                p = field.precision if field.precision else '10'
                s = field.scale if field.scale else '0'
                sql_type = f"DECIMAL({p}, {s})"
            elif sql_type == 'FLOAT':
                sql_type = "FLOAT"
            elif sql_type == 'DOUBLE':
                sql_type = "DOUBLE"
                
            col_def = f"`{field.name}` {sql_type}"
            columns_sql.append(col_def)
            
            if field.primaryKey:
                primary_keys.append(f"`{field.name}`")
                
        create_sql = f"CREATE TABLE `{request.table_name}` (\n"
        create_sql += ",\n".join(columns_sql)
        
        if primary_keys:
            create_sql += f",\nPRIMARY KEY ({', '.join(primary_keys)})"
            
        create_sql += "\n)"
        
        # StarRocks/Doris ç‰¹å®šè¯­æ³•
        is_olap = request.connector_type in ['starrocks', 'doris', 'starrocks-cdc', 'doris-cdc']
        if is_olap:
            create_sql += " ENGINE=OLAP"
            if primary_keys:
                create_sql += f"\nDISTRIBUTED BY HASH({', '.join(primary_keys)}) BUCKETS 10"
            else:
                # å¦‚æœæ²¡æœ‰ä¸»é”®ï¼ŒStarRocks éœ€è¦æŒ‡å®šåˆ†å¸ƒé”®
                if request.fields:
                    create_sql += f"\nDISTRIBUTED BY HASH(`{request.fields[0].name}`) BUCKETS 10"
            
            create_sql += "\nPROPERTIES(\"replication_num\" = \"1\")"

        logger.info(f"Executing Create Table SQL: {create_sql}")
        
        with engine.connect() as conn:
            conn.execute(text(create_sql))
            conn.commit()
            
        return {"status": "success", "message": f"è¡¨ {request.table_name} åˆ›å»ºæˆåŠŸ"}
        
    except Exception as e:
        logger.error(f"åˆ›å»ºè¡¨å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"åˆ›å»ºè¡¨å¤±è´¥: {str(e)}")

from .schemas import (
    SqlJobSubmitRequest, JarJobSubmitRequest, JobSubmitResponse,
    ClusterStatus, JobInfo, JobDetail, SavepointRequest, ResumeJobRequest
)
from .flink_client import flink_client, FlinkClient


# ============ å¥åº·æ£€æŸ¥ ============
@app.get("/health", tags=["ç³»ç»Ÿ"])
async def health_check():
    """ç³»ç»Ÿå¥åº·æ£€æŸ¥"""
    try:
        # æµ‹è¯•æ•°æ®åº“è¿æ¥
        session = db_manager.get_session()
        session.execute("SELECT 1")
        session.close()
        return {"status": "ok", "database": "connected"}
    except Exception as e:
        return {"status": "error", "database": "disconnected", "error": str(e)}


# ============ é›†ç¾¤ç®¡ç† ============
@app.get("/api/cluster/status", tags=["é›†ç¾¤ç®¡ç†"])
async def get_cluster_status():
    """è·å– Flink é›†ç¾¤çŠ¶æ€"""
    try:
        overview = await flink_client.get_cluster_overview()
        jobs_overview = await flink_client.get_jobs_overview()

        # ç»Ÿè®¡å„çŠ¶æ€ä½œä¸šæ•°é‡
        jobs_running = len([j for j in jobs_overview.get("jobs", []) if j.get("status") == "RUNNING"])
        jobs_finished = len([j for j in jobs_overview.get("jobs", []) if j.get("status") == "FINISHED"])
        jobs_cancelled = len([j for j in jobs_overview.get("jobs", []) if j.get("status") == "CANCELED"])
        jobs_failed = len([j for j in jobs_overview.get("jobs", []) if j.get("status") == "FAILED"])

        return {
            "status": "online",
            "flink_version": overview.get("flink-version", "unknown"),
            "slots_total": overview.get("slots-total", 0),
            "slots_available": overview.get("slots-available", 0),
            "taskmanagers": overview.get("taskmanagers", 0),
            "jobs_running": jobs_running,
            "jobs_finished": jobs_finished,
            "jobs_cancelled": jobs_cancelled,
            "jobs_failed": jobs_failed,
        }
    except Exception as e:
        return {
            "status": "offline",
            "error": str(e),
            "flink_version": None,
            "slots_total": 0,
            "slots_available": 0,
            "taskmanagers": 0,
            "jobs_running": 0,
            "jobs_finished": 0,
            "jobs_cancelled": 0,
            "jobs_failed": 0,
        }


@app.get("/api/cluster/config", tags=["é›†ç¾¤ç®¡ç†"])
async def get_cluster_config():
    """è·å– Flink é›†ç¾¤é…ç½®"""
    try:
        return await flink_client.get_cluster_config()
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/cluster/taskmanagers", tags=["é›†ç¾¤ç®¡ç†"])
async def get_taskmanagers():
    """è·å–æ‰€æœ‰ TaskManager åˆ—è¡¨"""
    try:
        return await flink_client.get_taskmanagers()
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/cluster/taskmanagers/{tm_id}", tags=["é›†ç¾¤ç®¡ç†"])
async def get_taskmanager_detail(tm_id: str):
    """è·å– TaskManager è¯¦æƒ…"""
    try:
        return await flink_client.get_taskmanager_detail(tm_id)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/cluster/taskmanagers/{tm_id}/metrics", tags=["é›†ç¾¤ç®¡ç†"])
async def get_taskmanager_metrics(tm_id: str, metrics: Optional[str] = Query(None, description="æŒ‡æ ‡åç§°ï¼Œé€—å·åˆ†éš”")):
    """è·å– TaskManager æŒ‡æ ‡"""
    try:
        metric_list = metrics.split(",") if metrics else None
        return await flink_client.get_taskmanager_metrics(tm_id, metric_list)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


# ============ ä½œä¸šç®¡ç† ============
@app.get("/api/jobs/history", tags=["ä½œä¸šç®¡ç†"])
async def get_job_history(limit: int = 50):
    """è·å–å†å²ä½œä¸šåˆ—è¡¨ï¼ˆåŒ…æ‹¬å·²åœæ­¢å’Œæ­£åœ¨è¿è¡Œçš„ä½œä¸šï¼ŒæŒ‰åç§°å»é‡ï¼‰"""
    logger.info(f"=== æ­£åœ¨è¯·æ±‚å†å²ä½œä¸šåˆ—è¡¨ (limit={limit}) ===")
    try:
        # ä»æ•°æ®åº“è·å–æ‰€æœ‰ä½œä¸š
        # æ³¨æ„ï¼šget_all_jobs ä¼šè¿”å›æ‰€æœ‰è®°å½•ï¼Œæˆ‘ä»¬éœ€è¦åœ¨è¿™é‡Œåšä¸¥æ ¼çš„å»é‡é€»è¾‘
        all_jobs = get_all_jobs(state=None, limit=limit)
        
        # ä» Flink è·å–æ­£åœ¨è¿è¡Œçš„ä½œä¸šåˆ—è¡¨
        flink_running_jobs = []
        flink_job_map = {}
        try:
            jobs_overview = await flink_client.get_jobs_overview()
            flink_jobs = jobs_overview.get("jobs", [])
            flink_running_jobs = [job["id"] for job in flink_jobs if job.get("status") == "RUNNING"]
            flink_job_map = {job["id"]: job for job in flink_jobs}
            logger.info(f"Flink ä¸­æœ‰ {len(flink_running_jobs)} ä¸ª RUNNING ä½œä¸š")
        except Exception as e:
            logger.warning(f"è·å– Flink ä½œä¸šåˆ—è¡¨å¤±è´¥: {e}")
        
        # è·å–æ‰€æœ‰æ­£åœ¨è¿è¡Œä½œä¸šçš„ resumed_from_job_idï¼ˆè¿™äº›å†å²ä½œä¸šå·²è¢«æˆåŠŸé‡å¯ï¼‰
        restarted_job_ids = set()
        for job in all_jobs:
            job_id = job.get("job_id")
            
            # å¦‚æœä½œä¸šåœ¨ Flink ä¸­å­˜åœ¨ï¼Œä½¿ç”¨ Flink çš„å®æ—¶ä¿¡æ¯æ›´æ–°
            if job_id in flink_job_map:
                flink_info = flink_job_map[job_id]
                # æ›´æ–°çŠ¶æ€
                if flink_info.get("status") == "RUNNING":
                    job["state"] = "RUNNING"
                
                # å¦‚æœæ•°æ®åº“ä¸­æ²¡æœ‰ start_timeï¼Œå°è¯•ä» Flink ä¿¡æ¯ä¸­è·å–
                if not job.get("start_time") and flink_info.get("start-time"):
                    job["start_time"] = flink_info.get("start-time")
            
            resumed_from = job.get("resumed_from_job_id")
            job_state = job.get("state")
            # å¦‚æœè¿™ä¸ªä½œä¸šæ­£åœ¨è¿è¡Œï¼Œå¹¶ä¸”æ˜¯ä»æŸä¸ªå†å²ä½œä¸šé‡å¯çš„
            if resumed_from and (job_state == "RUNNING" or job_id in flink_running_jobs):
                restarted_job_ids.add(resumed_from)
                logger.debug(f"ä½œä¸š {resumed_from} å·²è¢«æˆåŠŸé‡å¯ä¸º {job_id}")
        
        logger.info(f"å·²æˆåŠŸé‡å¯çš„å†å²ä½œä¸šæ•°: {len(restarted_job_ids)}")
        
        # è¿‡æ»¤é€»è¾‘ï¼š
        # 1. æ’é™¤å·²è¢«æˆåŠŸé‡å¯çš„æ—§ä½œä¸šï¼ˆå¦‚æœæ–°çš„å·²ç»åœ¨è¿è¡Œï¼‰
        # 2. ä¸å†æŒ‰åç§°å»é‡ï¼Œæ˜¾ç¤ºæ‰€æœ‰å†å²è®°å½•
        
        history_jobs = []

        # å…ˆæŒ‰åˆ›å»ºæ—¶é—´å€’åºæ’åºï¼Œç¡®ä¿å…ˆå¤„ç†æœ€æ–°çš„
        # æ³¨æ„ï¼šcreated_atå¯èƒ½æ˜¯Noneï¼Œéœ€è¦å¤„ç†
        sorted_all_jobs = sorted(
            all_jobs, 
            key=lambda x: x.get("created_at") or "",
            reverse=True
        )

        for job in sorted_all_jobs:
            job_id = job.get("job_id")
            
            # å¦‚æœè¿™ä¸ªä½œä¸šå·²ç»è¢«æˆåŠŸé‡å¯ï¼Œåˆ™ä¸æ˜¾ç¤ºï¼ˆå› ä¸ºå®ƒå·²ç»æ˜¯å†å²äº†ï¼Œæœ‰ä¸€ä¸ªæ–°çš„åœ¨è¿è¡Œï¼‰
            if job_id in restarted_job_ids:
                logger.debug(f"ä½œä¸š {job_id} å·²è¢«æˆåŠŸé‡å¯ï¼Œä¸æ˜¾ç¤ºåœ¨å†å²ä½œä¸šä¸­")
                continue
            
            history_jobs.append(job)
        
        # åŠ¨æ€æ›´æ–° RUNNING çŠ¶æ€ä½œä¸šçš„è¿è¡Œæ—¶é•¿
        current_time_ms = int(time.time() * 1000)
        for job in history_jobs:
            if job.get("state") == "RUNNING" and job.get("start_time"):
                try:
                    start_time = int(job.get("start_time"))
                    if start_time > 0:
                        job["duration"] = current_time_ms - start_time
                except (ValueError, TypeError):
                    pass

        # æœ€ç»ˆæ’åº
        try:
            sorted_jobs = sorted(
                history_jobs, 
                key=lambda x: x.get("created_at") or "",
                reverse=True
            )
        except Exception as sort_err:
            logger.error(f"æ’åºå†å²ä½œä¸šå¤±è´¥: {sort_err}")
            # é™çº§ï¼šå¦‚æœä¸æ’åºç›´æ¥è¿”å›
            sorted_jobs = history_jobs
        
        logger.info(f"è¿”å› {len(sorted_jobs)} ä¸ªå†å²ä½œä¸šï¼ˆä» {len(all_jobs)} ä¸ªä½œä¸šä¸­è¿‡æ»¤ï¼‰")
        return sorted_jobs
    except Exception as e:
        logger.error(f"Failed to get history jobs: {e}")
        import traceback
        logger.error(traceback.format_exc())
        raise HTTPException(status_code=500, detail=str(e))


@app.delete("/api/jobs/history/{job_id}", tags=["ä½œä¸šç®¡ç†"])
async def delete_history_job(job_id: str):
    """åˆ é™¤å†å²ä½œä¸šè®°å½•"""
    try:
        from .db_operations import delete_job
        success = delete_job(job_id)
        if success:
            logger.info(f"âœ… å·²åˆ é™¤ä½œä¸šè®°å½•: {job_id}")
            return {"status": "success", "message": f"ä½œä¸š {job_id} å·²åˆ é™¤"}
        else:
            raise HTTPException(status_code=404, detail=f"ä½œä¸š {job_id} ä¸å­˜åœ¨")
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"åˆ é™¤ä½œä¸šå¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/jobs/history/{job_id}/restart", tags=["ä½œä¸šç®¡ç†"])
async def restart_history_job(job_id: str, req: ResumeJobRequest):
    """ä»å†å²ä½œä¸šé‡æ–°å¯åŠ¨"""
    try:
        # ä½¿ç”¨URLä¸­çš„job_idï¼Œè¯·æ±‚ä½“ä¸­çš„job_idä¼šè¢«å¿½ç•¥
        job_id_to_restart = job_id
        
        # ä»æ•°æ®åº“è·å–ä½œä¸šä¿¡æ¯
        db_job = get_job(job_id_to_restart)
        
        if not db_job:
            raise HTTPException(
                status_code=404,
                detail={"error": "Job not found", "message": f"Cannot find job {job_id_to_restart} in database"}
            )
        
        sql_text = db_job.get("sql_text")
        job_name = db_job.get("job_name")
        flink_job_name = db_job.get("flink_job_name") # è·å–åŸå§‹ Flink Job Name
        parallelism = db_job.get("parallelism", 1)
        
        # è°ƒè¯•æ—¥å¿—ï¼šæ£€æŸ¥SQLå†…å®¹
        logger.info(f"=== æ¢å¤å†å²ä½œä¸š: {job_name} ===")
        logger.info(f"Job ID: {job_id_to_restart}")
        logger.info(f"SQLé•¿åº¦: {len(sql_text) if sql_text else 0}")
        logger.info(f"SQLå‰200å­—ç¬¦: {sql_text[:200] if sql_text else 'NULL'}")
        logger.info(f"æ˜¯å¦åŒ…å«INSERT: {'INSERT' in sql_text.upper() if sql_text else False}")
        
        # å¦‚æœSQLä¸ºç©ºæˆ–æ²¡æœ‰INSERTè¯­å¥ï¼Œå°è¯•ä»æ–‡ä»¶ä¸­è¯»å–
        if not sql_text or 'INSERT' not in sql_text.upper():
            logger.warning(f"âš ï¸ SQLä¸ºç©ºæˆ–æ²¡æœ‰INSERTè¯­å¥ï¼Œå°è¯•ä»æ–‡ä»¶ä¸­è¯»å–")
            sql_file_path = SQL_FILES_DIR / f"{job_id_to_restart}.sql"
            
            if sql_file_path.exists():
                sql_text = sql_file_path.read_text(encoding="utf-8")
                logger.info(f"âœ… ä»æ–‡ä»¶ä¸­è¯»å–åˆ°SQL: {sql_file_path}")
                logger.info(f"SQLé•¿åº¦: {len(sql_text)}")
                logger.info(f"æ˜¯å¦åŒ…å«INSERT: {'INSERT' in sql_text.upper()}")
            else:
                logger.warning(f"âš ï¸ SQLæ–‡ä»¶ä¸å­˜åœ¨: {sql_file_path}")
                
                # å°è¯•æ ¹æ®ä½œä¸šåç§°æŸ¥æ‰¾å…¶ä»–ä½œä¸šçš„SQL
                all_jobs = get_all_jobs(limit=100)
                for other_job in all_jobs:
                    if other_job.get("job_name") == job_name and other_job.get("sql_text"):
                        if 'INSERT' in other_job["sql_text"].upper():
                            sql_text = other_job["sql_text"]
                            logger.info(f"âœ… ä»ç›¸åŒåç§°çš„ä½œä¸šä¸­æ‰¾åˆ°SQL: {other_job.get('job_id')}")
                            break
                
                # å¦‚æœè¿˜æ˜¯æ²¡æ‰¾åˆ°ï¼Œå°è¯•éå†sql_jobsç›®å½•
                if not sql_text or 'INSERT' not in sql_text.upper():
                    for sql_file in SQL_FILES_DIR.glob("*.sql"):
                        content = sql_file.read_text(encoding="utf-8")
                        if 'INSERT' in content.upper():
                            name_file = SQL_FILES_DIR / f"{sql_file.stem}_name.txt"
                            if name_file.exists():
                                saved_name = name_file.read_text(encoding="utf-8").strip()
                                if saved_name == job_name:
                                    sql_text = content
                                    logger.info(f"âœ… æ ¹æ®ä½œä¸šåç§°åŒ¹é…åˆ°SQLæ–‡ä»¶: {sql_file}")
                                    break
        
        # å†æ¬¡æ£€æŸ¥SQLæ˜¯å¦åŒ…å«INSERTè¯­å¥
        if not sql_text or 'INSERT' not in sql_text.upper():
            raise HTTPException(
                status_code=400,
                detail={
                    "error": "Invalid SQL",
                    "message": "ä½œä¸šSQLä¸­æ²¡æœ‰INSERTè¯­å¥ï¼Œæ— æ³•æ¢å¤",
                    "hint": "è¯·æ£€æŸ¥ä½œä¸šæ˜¯å¦é€šè¿‡å¯è§†åŒ–é…ç½®æ­£ç¡®æäº¤ï¼Œæˆ–è€…æ‰‹åŠ¨æ·»åŠ INSERTè¯­å¥"
                }
            )
        
        # è·å–Savepointè·¯å¾„ï¼ˆä¼˜å…ˆä½¿ç”¨è¯·æ±‚ä¸­çš„ï¼‰
        savepoint_path = req.savepoint_path
        savepoint_timestamp = req.timestamp
        
        if not savepoint_path:
            # æŸ¥è¯¢æœ€æ–°çš„Savepoint
            latest_savepoint = get_latest_savepoint(job_id_to_restart)
            if latest_savepoint:
                savepoint_path = latest_savepoint.get("savepoint_path")
                savepoint_timestamp = latest_savepoint.get("timestamp")
                logger.info(f"Found latest savepoint: {savepoint_path}")
        
        # å¦‚æœæ²¡æœ‰æŒ‡å®štimestampï¼Œå°è¯•ä»db_jobè·å–ï¼ˆå¦‚æœè·¯å¾„åŒ¹é…ï¼‰
        if not savepoint_timestamp:
            if db_job and savepoint_path == db_job.get("savepoint_path"):
                 savepoint_timestamp = db_job.get("savepoint_timestamp")
            
            # è¡¥æ•‘æªæ–½ï¼šå¦‚æœè¿˜æ²¡è·å–åˆ°ï¼Œå°è¯•ä» savepoints è¡¨è·å–
            if not savepoint_timestamp and savepoint_path:
                try:
                    from .db_operations import db_manager, FlinkSavepoint
                    session = db_manager.get_session()
                    try:
                        sp_record = session.query(FlinkSavepoint).filter(
                            FlinkSavepoint.savepoint_path == savepoint_path
                        ).order_by(FlinkSavepoint.timestamp.desc()).first()
                        if sp_record:
                            savepoint_timestamp = sp_record.timestamp
                            logger.info(f"âœ… ä» savepoints è¡¨è¡¥å…¨ timestamp: {savepoint_timestamp}")
                    finally:
                        session.close()
                except Exception as e:
                    logger.warning(f"å°è¯•ä» savepoints è¡¨è·å– timestamp å¤±è´¥: {e}")

        # ç¡®ä¿ timestamp æ˜¯ int
        if savepoint_timestamp and isinstance(savepoint_timestamp, str):
            try:
                savepoint_timestamp = int(savepoint_timestamp)
            except:
                savepoint_timestamp = None

        # ä½¿ç”¨SQL Gatewayé‡æ–°æäº¤ä½œä¸š
        result = await flink_client.submit_sql_job(
            sql=sql_text,
            job_name=job_name,
            parallelism=parallelism,
            savepoint_path=savepoint_path
        )
        
        logger.info(f"submit_sql_job returned: {result}")
        
        job_id_new = result.get("jobid")
        operation_handle = result.get("operationHandle")
        
        if job_id_new:
            # æˆåŠŸè·å–åˆ°ä½œä¸šID
            logger.info(f"Job restarted successfully, new job ID: {job_id_new}")
            
            # è·å–å½“å‰æ—¶é—´ä½œä¸ºå¼€å§‹æ—¶é—´
            start_time = int(time.time() * 1000)

            # ä¿å­˜æ–°ä½œä¸šåˆ°æ•°æ®åº“
            save_job(
                job_id=job_id_new,
                job_name=job_name,
                sql_text=sql_text,
                parallelism=parallelism,
                flink_job_name=flink_job_name,
                resumed_from_job_id=job_id_to_restart,
                savepoint_path=savepoint_path,
                savepoint_timestamp=savepoint_timestamp,
                start_time=start_time,
                state="RUNNING"
            )
            
            # è®°å½•æ“ä½œæ—¥å¿—
            log_operation(
                job_id=job_id_new,
                operation_type="RESTART",
                operation_details={
                    "old_job_id": job_id_to_restart,
                    "savepoint_path": savepoint_path,
                    "savepoint_timestamp": savepoint_timestamp,
                    "source": "history"
                }
            )

            # å¦‚æœæ˜¯ä» savepoint å¯åŠ¨ï¼Œå°è¯•è®°å½• savepoint ä½¿ç”¨æƒ…å†µ
            if savepoint_path:
                try:
                    from .db_operations import save_savepoint
                    # å°è¯•ä»è·¯å¾„ä¸­æå– savepoint idï¼Œæˆ–è€…ç”Ÿæˆä¸€ä¸ª
                    sp_id = f"sp_{int(time.time())}"
                    save_savepoint(
                        job_id=job_id_new,
                        savepoint_id=sp_id,
                        savepoint_path=savepoint_path,
                        job_state="RESTORED",
                        timestamp=savepoint_timestamp
                    )
                except Exception as sp_err:
                    logger.warning(f"è®°å½• Savepoint ä½¿ç”¨æƒ…å†µå¤±è´¥: {sp_err}")
            
            return {
                "job_id": job_id_new,
                "old_job_id": job_id_to_restart,
                "savepoint_path": savepoint_path,
                "savepoint_timestamp": savepoint_timestamp,
                "status": "RUNNING",
                "message": "Job restarted from history"
            }
        elif operation_handle:
            # ä½œä¸šæ­£åœ¨å¯åŠ¨ä¸­ï¼Œç­‰å¾…ä¸€ä¸‹å°è¯•è·å– job_id
            logger.info(f"Job is starting, operation_handle: {operation_handle}, waiting for job_id...")
            
            import asyncio
            job_id_new = None
            
            # ç­‰å¾…å¹¶å°è¯•è·å– job_id
            for i in range(5):  # æœ€å¤šç­‰å¾… 5 ç§’
                await asyncio.sleep(1)
                try:
                    # ä» Flink REST API è·å–ä½œä¸šåˆ—è¡¨
                    jobs_overview = await flink_client.get_jobs_overview()
                    flink_jobs = jobs_overview.get("jobs", [])
                    
                    # æŸ¥æ‰¾ RUNNING çŠ¶æ€çš„æ–°ä½œä¸š
                    for job in flink_jobs:
                        if job.get("status") == "RUNNING":
                            potential_job_id = job.get("id")
                            # æ£€æŸ¥è¿™ä¸ª job_id æ˜¯å¦å·²ç»åœ¨æ•°æ®åº“ä¸­
                            existing_job = get_job(potential_job_id)
                            if not existing_job:
                                job_id_new = potential_job_id
                                logger.info(f"æ‰¾åˆ°æ–°å¯åŠ¨çš„ä½œä¸š: {job_id_new}")
                                break
                    
                    if job_id_new:
                        break
                except Exception as e:
                    logger.warning(f"ç­‰å¾…è·å– job_id å¤±è´¥: {e}")
            
            if job_id_new:
                # è·å–å½“å‰æ—¶é—´ä½œä¸ºå¼€å§‹æ—¶é—´
                start_time = int(time.time() * 1000)
                
                # ä¿å­˜æ–°ä½œä¸šåˆ°æ•°æ®åº“
                save_job(
                    job_id=job_id_new,
                    job_name=job_name,
                    sql_text=sql_text,
                    parallelism=parallelism,
                    flink_job_name=flink_job_name,
                    resumed_from_job_id=job_id_to_restart,
                    savepoint_path=savepoint_path,
                    savepoint_timestamp=savepoint_timestamp,
                    start_time=start_time,
                    state="RUNNING"
                )
                logger.info(f"âœ… æ–°ä½œä¸šå·²ä¿å­˜åˆ°æ•°æ®åº“: {job_id_new} - {job_name}")
                
                return {
                    "job_id": job_id_new,
                    "old_job_id": job_id_to_restart,
                    "savepoint_path": savepoint_path,
                    "status": "RUNNING",
                    "message": "Job restarted from history"
                }
            else:
                # æœªèƒ½è·å–åˆ° job_idï¼Œä½†ä½œä¸šå¯èƒ½å·²ç»å¯åŠ¨
                logger.warning(f"æœªèƒ½è·å–åˆ° job_idï¼Œä½†ä½œä¸šå¯èƒ½å·²å¯åŠ¨")
                raise HTTPException(
                    status_code=202,
                    detail={
                        "error": "Job is starting",
                        "message": "Job submitted but still starting, please refresh job list later",
                        "operation_handle": operation_handle,
                        "job_name": job_name,
                        "hint": "Job will appear in job list in a few seconds"
                    }
                )
        else:
            logger.error(f"Unexpected result: {result}")
            raise HTTPException(
                status_code=500,
                detail={
                    "error": "Failed to restart job",
                    "message": "Could not get new job ID or operation handle",
                    "debug_info": result
                }
            )
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to restart history job: {e}")
        import traceback
        logger.error(traceback.format_exc())
        raise HTTPException(
            status_code=500,
            detail={
                "error": "Failed to restart history job",
                "message": str(e)
            }
        )


@app.get("/api/jobs", tags=["ä½œä¸šç®¡ç†"])
async def get_jobs():
    """è·å–å®æ—¶ä½œä¸šåˆ—è¡¨ï¼ˆä»Flinkæ¥å£è·å–ï¼Œåªæ˜¾ç¤ºRUNNINGçŠ¶æ€çš„ä½œä¸šï¼‰"""
    try:
        # ä» Flink è·å–ä½œä¸šåˆ—è¡¨
        jobs_overview = await flink_client.get_jobs_overview()
        flink_jobs = jobs_overview.get("jobs", [])
        
        logger.info(f"ä» Flink è·å–åˆ° {len(flink_jobs)} ä¸ªä½œä¸š")
        
        # è½¬æ¢å­—æ®µåä»¥å…¼å®¹å‰ç«¯
        converted_jobs = []
        for flink_job in flink_jobs:
            job_id = flink_job.get("id")
            state = flink_job.get("status")
            
            # åªæ˜¾ç¤º RUNNING çŠ¶æ€çš„ä½œä¸šï¼ˆå®æ—¶ä½œä¸šï¼‰
            if state != "RUNNING":
                continue
            
            # ä»æ•°æ®åº“è·å–ä½œä¸šä¿¡æ¯ï¼ˆç¡®ä¿èƒ½è·å–åˆ°ï¼Œä¸å—åˆ—è¡¨æ•°é‡é™åˆ¶ï¼‰
            db_job = get_job(job_id)
            
            # ç”¨æˆ·é…ç½®çš„ä½œä¸šåç§°ï¼ˆä»æ•°æ®åº“ï¼‰
            user_job_name = db_job.get("job_name") if db_job else None
            
            # ä¼˜å…ˆä» Flink è·å–å®æ—¶ä¿¡æ¯ï¼ˆæ»¡è¶³ç”¨æˆ·éœ€æ±‚ï¼šå…¶ä»–ä¿¡æ¯æ˜¯ä»flinkæ¥å£è·å–çš„ï¼‰
            # 1. å°è¯•ä»åˆ—è¡¨ä¿¡æ¯ä¸­è·å–
            start_time = flink_job.get("start-time", 0)
            end_time = flink_job.get("end-time", 0)
            duration = flink_job.get("duration", 0)
            flink_job_name = flink_job.get("name", job_id)

            # 2. å¦‚æœåˆ—è¡¨ä¿¡æ¯ä¸å…¨ï¼ˆä¾‹å¦‚ä½¿ç”¨äº†ç®€ç‰ˆ /jobs æ¥å£ï¼‰ï¼Œåˆ™è·å–è¯¦æƒ…
            if start_time == 0:
                try:
                    job_detail = await flink_client.get_job_detail(job_id)
                    flink_job_name = job_detail.get("name", job_id)
                    start_time = job_detail.get("start-time", 0) or 0
                    end_time = job_detail.get("end-time", 0) or 0
                    duration = job_detail.get("duration", 0) or 0
                    logger.debug(f"ä» Flink è¯¦æƒ…è·å–: {job_id} - {flink_job_name}, start_time={start_time}")
                except Exception as e:
                    logger.warning(f"è·å–ä½œä¸š {job_id} è¯¦æƒ…å¤±è´¥: {e}")

            if db_job:
                # æ•°æ®åº“åªæä¾›ç”¨æˆ·é…ç½®çš„é™æ€ä¿¡æ¯
                user_job_name = db_job.get("job_name")
                savepoint_path = db_job.get("savepoint_path")
                sql_text = db_job.get("sql_text")
                logger.debug(f"From DB: {job_id} - user_name: {user_job_name}")
            else:
                # ä½œä¸šä¸åœ¨æ•°æ®åº“ä¸­
                logger.info(f"ä½œä¸š {job_id} ä¸åœ¨æ•°æ®åº“ä¸­")
                savepoint_path = None
                sql_text = None
            
            # æ£€æŸ¥å¹¶æ›´æ–°æ•°æ®åº“ä¸­çš„ flink_job_name
            actual_flink_name = flink_job.get("name", job_id)
            if db_job:
                db_flink_name = db_job.get("flink_job_name")
                # å¦‚æœæ•°æ®åº“ä¸­æ²¡æœ‰ flink_job_name æˆ–è€…ä¸å®é™…ä¸ä¸€è‡´ï¼ˆä¸”å®é™…åç§°ä¸æ˜¯job_idï¼‰ï¼Œåˆ™æ›´æ–°
                if (not db_flink_name or db_flink_name != actual_flink_name) and actual_flink_name != job_id:
                     logger.info(f"åŒæ­¥ flink_job_name: {job_id} - {db_flink_name} -> {actual_flink_name}")
                     # å¼‚æ­¥æ›´æ–°æ•°æ®åº“ï¼ˆè¿™é‡Œç®€å•èµ·è§ç›´æ¥è°ƒç”¨åŒæ­¥æ–¹æ³•ï¼Œå› ä¸ºæ˜¯å°‘é‡æ“ä½œï¼‰
                     try:
                         from .db_operations import save_job
                         # åªæ›´æ–° flink_job_nameï¼Œå…¶ä»–ä¿æŒä¸å˜
                         # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦ä¼ å…¥ job_nameï¼Œå¦åˆ™æ— æ³•å®šä½ï¼ˆå¦‚æœæŒ‰åç§°æ›´æ–°ï¼‰æˆ–è€…ä¼šæ›´æ–°ä¸ºç©º
                         # ç”±äº save_job ç°åœ¨çš„é€»è¾‘æ¯”è¾ƒå¤æ‚ï¼Œæˆ‘ä»¬æœ€å¥½åªæ›´æ–° job_id å¯¹åº”çš„è®°å½•
                         # è¿™é‡Œä¸ºäº†å®‰å…¨ï¼Œæˆ‘ä»¬åªåœ¨ db_job å­˜åœ¨æ—¶æ›´æ–°
                         user_job_name_for_update = db_job.get("job_name")
                         if user_job_name_for_update:
                            save_job(
                                job_id=job_id,
                                job_name=user_job_name_for_update,
                                sql_text="", # ä¸æ›´æ–° SQL
                                flink_job_name=actual_flink_name,
                                parallelism=None, # ä¸æ›´æ–°å¹¶è¡Œåº¦
                                update_by_name=False # å¼ºåˆ¶æŒ‰ ID æ›´æ–°ï¼Œé˜²æ­¢å‰¯ä½œç”¨
                            )
                            # æ›´æ–°å†…å­˜ä¸­çš„ db_job ä¿¡æ¯ï¼Œä»¥ä¾¿åç»­é€»è¾‘ä½¿ç”¨æœ€æ–°å€¼
                            db_job["flink_job_name"] = actual_flink_name
                            flink_job_name = actual_flink_name
                     except Exception as update_err:
                         logger.error(f"åŒæ­¥ flink_job_name å¤±è´¥: {update_err}")

            # åŠ¨æ€è®¡ç®—è¿è¡Œæ—¶é•¿ï¼šå¦‚æœ Flink æ²¡æœ‰è¿”å› durationï¼Œåˆ™æ‰‹åŠ¨è®¡ç®—
            # ä¸¥æ ¼éµå®ˆç”¨æˆ·è¦æ±‚ï¼šå…¶ä»–ä¿¡æ¯æ˜¯ä»flinkæ¥å£è·å–çš„
            if duration == 0 and start_time > 0:
                current_time_ms = int(time.time() * 1000)
                duration = current_time_ms - start_time
            
            # æ ¹æ®ç”¨æˆ·è¦æ±‚ï¼Œä½œä¸šåç§°ä¼˜å…ˆä» flink_job è¡¨çš„ job_name è·å–
            # display_name ç”¨äºå‰ç«¯å±•ç¤ºï¼Œä½†ä¸ºäº†å…¼å®¹æ€§ï¼Œæˆ‘ä»¬å°†åˆ†åˆ«è¿”å› name (Flinkåç§°) å’Œ user_job_name (ç”¨æˆ·åç§°)
            # display_name = user_job_name if user_job_name else flink_job_name

            converted_job = {
                "jid": job_id,
                "state": state,
                "name": flink_job_name,  # æ¢å¤ä¸º Flink ä½œä¸šåç§° (e.g. insert-into...)
                "flink_name": flink_job_name,  # ä¿ç•™ Flink åŸå§‹åç§°
                "user_job_name": user_job_name,  # ç”¨æˆ·é…ç½®çš„ä½œä¸šåç§°
                "sql_text": sql_text, # ä½œä¸šSQL
                "start-time": start_time,
                "end-time": end_time,
                "duration": duration,
                "savepoint_path": savepoint_path
            }
            converted_jobs.append(converted_job)

        # æŒ‰ start_time é™åºæ’åºï¼ˆæœ€æ–°çš„åœ¨å‰é¢ï¼‰ï¼Œstart_time ä¸º 0 çš„æ’åœ¨æœ€å
        # ä½¿ç”¨å®‰å…¨çš„æ¯”è¾ƒæ–¹å¼ï¼Œé¿å… None å€¼å¯¼è‡´é”™è¯¯
        try:
            converted_jobs.sort(key=lambda x: (x.get("start-time") or 0), reverse=True)
        except Exception as sort_err:
            logger.error(f"æ’åºå®æ—¶ä½œä¸šå¤±è´¥: {sort_err}")
        
        logger.info(f"è¿”å› {len(converted_jobs)} ä¸ªå®æ—¶ä½œä¸šï¼ˆRUNNING çŠ¶æ€ï¼‰")
        return converted_jobs
    except Exception as e:
        logger.error(f"è·å–ä½œä¸šåˆ—è¡¨å¤±è´¥: {e}")
        import traceback
        logger.error(traceback.format_exc())
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/jobs/{job_id}", tags=["ä½œä¸šç®¡ç†"])
async def get_job_detail(job_id: str):
    """è·å–ä½œä¸šè¯¦æƒ…ï¼ˆä»Flink APIç›´æ¥è¿”å›ï¼‰"""
    try:
        job_detail = await flink_client.get_job_detail(job_id)
        return job_detail
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/jobs/{job_id}/plan", tags=["ä½œä¸šç®¡ç†"])
async def get_job_plan(job_id: str):
    """è·å–ä½œä¸šæ‰§è¡Œè®¡åˆ’"""
    try:
        return await flink_client.get_job_plan(job_id)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/jobs/{job_id}/exceptions", tags=["ä½œä¸šç®¡ç†"])
async def get_job_exceptions(job_id: str):
    """è·å–ä½œä¸šå¼‚å¸¸ä¿¡æ¯"""
    try:
        return await flink_client.get_job_exceptions(job_id)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/jobs/{job_id}/config", tags=["ä½œä¸šç®¡ç†"])
async def get_job_config(job_id: str):
    """è·å–ä½œä¸šé…ç½®"""
    try:
        return await flink_client.get_job_config(job_id)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/jobs/{job_id}/db-detail", tags=["ä½œä¸šç®¡ç†"])
async def get_job_db_detail(job_id: str):
    """è·å–æ•°æ®åº“ä¸­çš„ä½œä¸šè¯¦æƒ…ï¼ˆä¸åŒ…å«IDåˆ—ï¼‰"""
    try:
        job = get_job(job_id)
        if not job:
            raise HTTPException(status_code=404, detail=f"Job {job_id} not found in database")
        
        # ç§»é™¤ id å­—æ®µ
        if "id" in job:
            del job["id"]
            
        return job
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"è·å–ä½œä¸šæ•°æ®åº“è¯¦æƒ…å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/jobs/{job_id}/cancel", tags=["ä½œä¸šç®¡ç†"])
async def cancel_job(job_id: str):
    """å–æ¶ˆä½œä¸š"""
    import time
    try:
        await flink_client.cancel_job(job_id)
        
        # è·å–å½“å‰æ—¶é—´ä½œä¸ºç»“æŸæ—¶é—´
        end_time = int(time.time() * 1000)
        
        # è·å–ä½œä¸šå¼€å§‹æ—¶é—´æ¥è®¡ç®—è¿è¡Œæ—¶é•¿
        db_job = get_job(job_id)
        duration = None
        if db_job:
            start_time = db_job.get("start_time")
            if start_time and start_time > 0:
                duration = end_time - start_time
                logger.info(f"ä½œä¸š {job_id} è¿è¡Œæ—¶é•¿: {duration}ms")
        
        # æ›´æ–°æ•°æ®åº“ä¸­çš„ä½œä¸šçŠ¶æ€
        update_job_state(job_id, "CANCELED", end_time=end_time, duration=duration)
        
        # è®°å½•æ“ä½œæ—¥å¿—
        log_operation(job_id=job_id, operation_type="CANCEL")
        
        return {"job_id": job_id, "status": "CANCELED"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/jobs/{job_id}/stop", tags=["ä½œä¸šç®¡ç†"])
async def stop_job(job_id: str, req: SavepointRequest = SavepointRequest()):
    """æš‚åœä½œä¸š"""
    import time
    try:
        logger.info(f"=== å¼€å§‹æš‚åœä½œä¸š: {job_id} ===")
        logger.info(f"è¯·æ±‚å‚æ•°: target_directory={req.target_directory}, withSavepoint={req.withSavepoint}")
        
        try:
            if req.target_directory or req.withSavepoint:
                logger.info(f"è°ƒç”¨stop_job_with_savepointï¼Œç›®å½•: {req.target_directory}")
                result = await flink_client.stop_job_with_savepoint(job_id, req.target_directory)
                
                logger.info(f"Flink APIè¿”å›: {result}")
                
                # ä¿å­˜savepointè·¯å¾„åˆ°æ–‡ä»¶ï¼Œä»¥ä¾¿åç»­æ¢å¤
                savepoint_path = result.get("savepoint_path")
                savepoint_id = result.get("savepoint_id", job_id)
                savepoint_timestamp = result.get("savepoint_timestamp")
                
                # è·å–å½“å‰æ—¶é—´ä½œä¸ºç»“æŸæ—¶é—´
                end_time = int(time.time() * 1000)
                
                # å¦‚æœæ²¡æœ‰ä»Flinkè·å–åˆ°savepointæ—¶é—´æˆ³ï¼Œä½¿ç”¨å½“å‰æ—¶é—´
                if not savepoint_timestamp:
                    savepoint_timestamp = end_time
                    logger.info(f"âš ï¸ ä½¿ç”¨å½“å‰æ—¶é—´ä½œä¸º savepoint_timestamp: {savepoint_timestamp}")
                else:
                    logger.info(f"âœ… ä½¿ç”¨ Flink è¿”å›çš„ savepoint_timestamp: {savepoint_timestamp}")
                
                # è·å–ä½œä¸šå¼€å§‹æ—¶é—´æ¥è®¡ç®—è¿è¡Œæ—¶é•¿
                db_job = get_job(job_id)
                duration = None
                start_time = None
                
                if db_job:
                    start_time = db_job.get("start_time")
                
                # å¦‚æœæ•°æ®åº“ä¸­æ²¡æœ‰ start_timeï¼Œä» Flink è·å–
                if not start_time or start_time <= 0:
                    try:
                        job_detail = await flink_client.get_job_detail(job_id)
                        start_time = job_detail.get("start-time", 0)
                        logger.info(f"ä» Flink è·å–ä½œä¸šå¼€å§‹æ—¶é—´: {start_time}")
                    except Exception as e:
                        logger.warning(f"è·å–ä½œä¸šè¯¦æƒ…å¤±è´¥: {e}")
                
                if start_time and start_time > 0:
                    duration = end_time - start_time
                    logger.info(f"ä½œä¸š {job_id} è¿è¡Œæ—¶é•¿: {duration}ms")
                
                if savepoint_path:
                    savepoint_file = SQL_FILES_DIR / f"{job_id}_savepoint.txt"
                    savepoint_file.write_text(savepoint_path, encoding="utf-8")
                    logger.info(f"âœ… Savepointè·¯å¾„å·²ä¿å­˜åˆ°æ–‡ä»¶: {savepoint_file}")
                    
                    # ä¿å­˜Savepointä¿¡æ¯åˆ°æ•°æ®åº“
                    save_success = save_savepoint(
                        job_id=job_id,
                        savepoint_id=savepoint_id,
                        savepoint_path=savepoint_path,
                        job_state="FINISHED",
                        timestamp=savepoint_timestamp  # ä½¿ç”¨Savepointæ—¶é—´æˆ³
                    )
                    if save_success:
                        logger.info(f"âœ… Savepointä¿¡æ¯å·²ä¿å­˜åˆ°æ•°æ®åº“: {savepoint_id}")
                    else:
                        logger.error(f"âŒ ä¿å­˜Savepointåˆ°æ•°æ®åº“å¤±è´¥ï¼Œä½†ç»§ç»­æ›´æ–°ä½œä¸šçŠ¶æ€")
                
                # æ›´æ–°ä½œä¸šçŠ¶æ€ï¼ŒåŒæ—¶ä¿å­˜ start_time å’Œ savepoint_path
                # ä½¿ç”¨ update_job_state ç›´æ¥æ›´æ–°æ‰€æœ‰ä¿¡æ¯
                update_success = update_job_state(
                    job_id=job_id, 
                    state="STOPPED", 
                    start_time=start_time, 
                    end_time=end_time, 
                    duration=duration,
                    savepoint_path=savepoint_path,
                    savepoint_timestamp=savepoint_timestamp
                )
                
                if update_success:
                    logger.info(f"âœ… ä½œä¸šçŠ¶æ€å·²æ›´æ–°: {job_id} -> STOPPED, savepoint={savepoint_path}, timestamp={savepoint_timestamp}")
                    
                    # å†æ¬¡éªŒè¯æ•°æ®åº“ä¸­çš„æ•°æ®
                    saved_job = get_job(job_id)
                    if saved_job:
                        logger.info(f"ğŸ” æ•°æ®åº“éªŒè¯: path={saved_job.get('savepoint_path')}, ts={saved_job.get('savepoint_timestamp')}")
                else:
                    logger.error(f"âŒ æ›´æ–°ä½œä¸šçŠ¶æ€å¤±è´¥")
                
                # è®°å½•æ“ä½œæ—¥å¿—
                log_operation(
                    job_id=job_id,
                    operation_type="STOP",
                    operation_details={
                        "savepoint_path": savepoint_path,
                        "target_directory": req.target_directory
                    }
                )
                
                logger.info(f"=== æš‚åœä½œä¸šå®Œæˆ: {job_id} ===")
                return result
            else:
                logger.info(f"ä½œä¸š {job_id} ä¸å¸¦savepointæš‚åœ")
                await flink_client.stop_job(job_id)
                
                # è·å–å½“å‰æ—¶é—´ä½œä¸ºç»“æŸæ—¶é—´
                end_time = int(time.time() * 1000)
                
                # è·å–ä½œä¸šå¼€å§‹æ—¶é—´æ¥è®¡ç®—è¿è¡Œæ—¶é•¿
                db_job = get_job(job_id)
                duration = None
                start_time = None
                
                if db_job:
                    start_time = db_job.get("start_time")
                
                # å¦‚æœæ•°æ®åº“ä¸­æ²¡æœ‰ start_timeï¼Œä» Flink è·å–
                if not start_time or start_time <= 0:
                    try:
                        job_detail = await flink_client.get_job_detail(job_id)
                        start_time = job_detail.get("start-time", 0)
                        logger.info(f"ä» Flink è·å–ä½œä¸šå¼€å§‹æ—¶é—´: {start_time}")
                    except Exception as e:
                        logger.warning(f"è·å–ä½œä¸šè¯¦æƒ…å¤±è´¥: {e}")
                
                if start_time and start_time > 0:
                    duration = end_time - start_time
                    logger.info(f"ä½œä¸š {job_id} è¿è¡Œæ—¶é•¿: {duration}ms")
                
                # æ›´æ–°ä½œä¸šçŠ¶æ€ï¼ŒåŒæ—¶ä¿å­˜ start_time
                update_success = update_job_state(job_id, "STOPPED", start_time=start_time, end_time=end_time, duration=duration)
                if update_success:
                    logger.info(f"âœ… ä½œä¸šçŠ¶æ€å·²æ›´æ–°: {job_id} -> STOPPED")
                else:
                    logger.error(f"âŒ æ›´æ–°ä½œä¸šçŠ¶æ€å¤±è´¥")
                
                log_operation(job_id=job_id, operation_type="STOP")
                return {"job_id": job_id, "status": "STOPPED"}
                
        except Exception as flink_error:
            # ç‰¹æ®Šå¤„ç†ï¼šå¦‚æœFlinkæŠ¥é”™è¯´æ‰¾ä¸åˆ°ä½œä¸šï¼Œè¯´æ˜ä½œä¸šå·²ç»åœæ­¢æˆ–ä¸å­˜åœ¨
            error_str = str(flink_error)
            if "404" in error_str or "NotFound" in error_str:
                logger.warning(f"Flink ä¸­æœªæ‰¾åˆ°ä½œä¸š {job_id} (å¯èƒ½å·²åœæ­¢): {flink_error}")
                # å¼ºåˆ¶æ›´æ–°æ•°æ®åº“çŠ¶æ€ä¸º STOPPED
                update_job_state(job_id, "STOPPED", end_time=int(time.time() * 1000))
                logger.info(f"âœ… å·²å¼ºåˆ¶æ›´æ–°æ•°æ®åº“çŠ¶æ€ä¸º STOPPED: {job_id}")
                return {"job_id": job_id, "status": "STOPPED", "message": "Job not found in Flink, marked as STOPPED in DB"}
            else:
                raise flink_error
                
    except Exception as e:
        logger.error(f"æš‚åœä½œä¸šå¤±è´¥: {e}")
        import traceback
        logger.error(traceback.format_exc())
        raise HTTPException(status_code=500, detail=str(e))


# ============ å…¨å±€å˜é‡ ============
# ç”¨äºé˜²æ­¢é‡å¤æäº¤
_job_restart_locks = {}  # {job_id: timestamp}


async def check_and_set_job_lock(job_id: str) -> bool:
    """æ£€æŸ¥å¹¶è®¾ç½®ä½œä¸šé”ï¼Œé˜²æ­¢é‡å¤æäº¤"""
    import time
    current_time = time.time()
    
    if job_id in _job_restart_locks:
        last_restart_time = _job_restart_locks[job_id]
        if current_time - last_restart_time < 30:  # 30ç§’å†…ä¸å…è®¸é‡å¤æäº¤
            logger.warning(f"ä½œä¸š {job_id} åœ¨30ç§’å†…å·²é‡å¯è¿‡ï¼Œæ‹’ç»é‡å¤è¯·æ±‚")
            return False
    
    _job_restart_locks[job_id] = current_time
    return True


@app.post("/api/jobs/{job_id}/restart", tags=["ä½œä¸šç®¡ç†"])
async def restart_job(job_id: str, req: ResumeJobRequest):
    """é‡å¯ä½œä¸šï¼ˆä»savepointæ¢å¤ï¼‰"""
    # æ£€æŸ¥é˜²é‡å¤æäº¤
    if not await check_and_set_job_lock(job_id):
        raise HTTPException(
            status_code=429,
            detail={
                "error": "Too many requests",
                "message": "ä½œä¸šæ­£åœ¨æ¢å¤ä¸­ï¼Œè¯·å‹¿é‡å¤ç‚¹å‡»"
            }
        )
    
    try:
        # ä¼˜å…ˆä»æ•°æ®åº“è¯»å–ä½œä¸šä¿¡æ¯
        db_job = get_job(job_id)
        
        savepoint_timestamp = req.timestamp
        if savepoint_timestamp and isinstance(savepoint_timestamp, str):
            try:
                savepoint_timestamp = int(savepoint_timestamp)
            except:
                savepoint_timestamp = None
        
        sql_text = None
        original_job_name = None
        parallelism = 1
        
        if db_job:
            # ä»æ•°æ®åº“è¯»å–ä½œä¸šä¿¡æ¯
            sql_text = db_job.get("sql_text")
            original_job_name = db_job.get("job_name")
            parallelism = db_job.get("parallelism", 1)
            logger.info(f"âœ… ä»æ•°æ®åº“è¯»å–åˆ°ä½œä¸šä¿¡æ¯: {job_id} - {original_job_name}")
            logger.info(f"SQLé•¿åº¦: {len(sql_text) if sql_text else 0}")
            
            # æ£€æŸ¥SQLæ˜¯å¦æœ‰æ•ˆï¼Œå¦‚æœä¸ºç©ºæˆ–æ²¡æœ‰INSERTï¼Œå°è¯•ä»æ–‡ä»¶è¯»å–
            if not sql_text or 'INSERT' not in sql_text.upper():
                logger.warning(f"âš ï¸ æ•°æ®åº“ä¸­çš„SQLä¸ºç©ºæˆ–æ²¡æœ‰INSERTè¯­å¥ï¼Œå°è¯•ä»æ–‡ä»¶è¯»å–")
                sql_file_path = SQL_FILES_DIR / f"{job_id}.sql"
                if sql_file_path.exists():
                    sql_text = sql_file_path.read_text(encoding="utf-8")
                    logger.info(f"âœ… ä»æ–‡ä»¶è¯»å–åˆ°SQL: {sql_file_path}")
                    logger.info(f"SQLé•¿åº¦: {len(sql_text)}")
                else:
                    logger.warning(f"âš ï¸ SQLæ–‡ä»¶ä¸å­˜åœ¨: {sql_file_path}")
                    # å°è¯•æ ¹æ®ä½œä¸šåç§°æŸ¥æ‰¾å…¶ä»–ä½œä¸šçš„SQL
                    from .db_operations import get_all_jobs
                    all_jobs = get_all_jobs(limit=100)
                    for other_job in all_jobs:
                        if other_job.get("job_name") == original_job_name and other_job.get("sql_text"):
                            if 'INSERT' in other_job["sql_text"].upper():
                                sql_text = other_job["sql_text"]
                                logger.info(f"âœ… ä»ç›¸åŒåç§°çš„ä½œä¸šä¸­æ‰¾åˆ°SQL: {other_job.get('job_id')}")
                                break
                    
                    # å¦‚æœè¿˜æ˜¯æ²¡æ‰¾åˆ°ï¼Œå°è¯•éå†sql_jobsç›®å½•æŒ‰ä½œä¸šåç§°åŒ¹é…
                    if not sql_text or 'INSERT' not in sql_text.upper():
                        logger.info(f"ğŸ” éå†sql_jobsç›®å½•æŸ¥æ‰¾ä½œä¸šåç§°: {original_job_name}")
                        for sql_file in SQL_FILES_DIR.glob("*.sql"):
                            content = sql_file.read_text(encoding="utf-8")
                            if 'INSERT' in content.upper():
                                name_file = SQL_FILES_DIR / f"{sql_file.stem}_name.txt"
                                if name_file.exists():
                                    saved_name = name_file.read_text(encoding="utf-8").strip()
                                    logger.debug(f"  æ£€æŸ¥: {sql_file.name} -> {saved_name}")
                                    if saved_name == original_job_name:
                                        sql_text = content
                                        logger.info(f"âœ… æ ¹æ®ä½œä¸šåç§°åŒ¹é…åˆ°SQLæ–‡ä»¶: {sql_file}")
                                        break
                    
                    # ç»ˆæå¤‡ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨æœ€æ–°çš„åŒ…å«INSERTçš„SQLæ–‡ä»¶
                    if not sql_text or 'INSERT' not in sql_text.upper():
                        logger.warning(f"âš ï¸ æŒ‰åç§°æœªæ‰¾åˆ°ï¼Œå°è¯•ä½¿ç”¨æœ€æ–°çš„SQLæ–‡ä»¶")
                        sql_files = list(SQL_FILES_DIR.glob("*.sql"))
                        sql_files.sort(key=lambda f: f.stat().st_mtime, reverse=True)
                        for sql_file in sql_files:
                            content = sql_file.read_text(encoding="utf-8")
                            if 'INSERT' in content.upper():
                                sql_text = content
                                logger.info(f"âœ… ä½¿ç”¨æœ€æ–°çš„SQLæ–‡ä»¶: {sql_file.name}")
                                # åŒæ—¶è¯»å–å¯¹åº”çš„ä½œä¸šåç§°
                                name_file = SQL_FILES_DIR / f"{sql_file.stem}_name.txt"
                                if name_file.exists():
                                    original_job_name = name_file.read_text(encoding="utf-8").strip()
                                break
        else:
            # é™çº§åˆ°ä»æ–‡ä»¶è¯»å–
            sql_file_path = SQL_FILES_DIR / f"{job_id}.sql"
            
            if not sql_file_path.exists():
                # å°è¯•ä»å…¶ä»–å¯èƒ½çš„æ–‡ä»¶åè¯»å–
                alternate_files = list(SQL_FILES_DIR.glob("*.sql"))
                if alternate_files:
                    latest_sql = max(alternate_files, key=lambda f: f.stat().st_mtime)
                    logger.warning(f"æœªæ‰¾åˆ° {job_id}.sqlï¼Œä½¿ç”¨æœ€è¿‘ä¿®æ”¹çš„æ–‡ä»¶: {latest_sql.name}")
                    sql_file_path = latest_sql
                else:
                    raise HTTPException(
                        status_code=404,
                        detail={
                            "error": "æœªæ‰¾åˆ°ä½œä¸šä¿¡æ¯",
                            "message": f"æ— æ³•ä»æ•°æ®åº“æˆ–æ–‡ä»¶æ‰¾åˆ°ä½œä¸š {job_id} çš„ä¿¡æ¯ã€‚"
                        }
                    )
            
            sql_text = sql_file_path.read_text(encoding="utf-8")
            logger.info(f"âœ… ä»æ–‡ä»¶è¯»å–åˆ°ä½œä¸šSQL: {sql_file_path}")
            
            # è¯»å–ä½œä¸šåç§°
            name_file_path = SQL_FILES_DIR / f"{job_id}_name.txt"
            if name_file_path.exists():
                original_job_name = name_file_path.read_text(encoding="utf-8").strip()
        
        # å¦‚æœä»ç„¶æ²¡æœ‰æ‰¾åˆ°ä½œä¸šåç§°ï¼Œä½¿ç”¨job_id
        if not original_job_name:
            original_job_name = job_id
            logger.warning(f"æœªæ‰¾åˆ°ä½œä¸šåç§°ï¼Œä½¿ç”¨job_id: {job_id}")
        
        # è·å–Savepointè·¯å¾„ï¼ˆä¼˜å…ˆä½¿ç”¨è¯·æ±‚ä¸­çš„ï¼Œå¦åˆ™ä»æ•°æ®åº“æŸ¥è¯¢æœ€æ–°çš„ï¼‰
        savepoint_path = req.savepoint_path
        if not savepoint_path:
            # è·å–Savepointè·¯å¾„ï¼ˆä¼˜å…ˆä½¿ç”¨è¯·æ±‚ä¸­çš„ï¼Œå¦åˆ™ä»æ•°æ®åº“æŸ¥è¯¢æœ€æ–°çš„ï¼‰
            latest_savepoint = get_latest_savepoint(job_id)
            if latest_savepoint:
                savepoint_path = latest_savepoint.get("savepoint_path")
                if not req.timestamp:
                    savepoint_timestamp = latest_savepoint.get("timestamp")
                logger.info(f"âœ… ä»æ•°æ®åº“è·å–åˆ°æœ€æ–°Savepoint: {savepoint_path}")
            else:
                logger.warning(f"âš ï¸ æ•°æ®åº“ä¸­æœªæ‰¾åˆ° job_id={job_id} çš„Savepoint")
                
                # å°è¯•æœç´¢æ‰€æœ‰savepointsï¼ŒæŸ¥æ‰¾å¯èƒ½åŒ¹é…çš„
                all_savepoints = get_savepoints(limit=100)
                logger.info(f"æ•°æ®åº“ä¸­æ‰€æœ‰Savepointæ•°é‡: {len(all_savepoints)}")
                for sp in all_savepoints:
                    logger.debug(f"  - Savepoint: job_id={sp.get('job_id')}, path={sp.get('savepoint_path')}")
                
                # é™çº§åˆ°ä»æ–‡ä»¶è¯»å–
                savepoint_file = SQL_FILES_DIR / f"{job_id}_savepoint.txt"
                if savepoint_file.exists():
                    savepoint_path = savepoint_file.read_text(encoding="utf-8").strip()
                    logger.info(f"âœ… ä»æ–‡ä»¶è¯»å–åˆ°Savepoint: {savepoint_path}")
                else:
                    logger.warning(f"âš ï¸ æ–‡ä»¶ä¹Ÿä¸å­˜åœ¨: {savepoint_file}")
                    
                    # éå†æ‰€æœ‰savepointæ–‡ä»¶
                    for sp_file in SQL_FILES_DIR.glob("*_savepoint.txt"):
                        logger.debug(f"  - å‘ç°savepointæ–‡ä»¶: {sp_file.name}")
        
        # å¦‚æœè¿˜æ˜¯æ‰¾ä¸åˆ°savepointï¼Œè­¦å‘Šä½†å…è®¸ç»§ç»­ï¼ˆä¸ä½¿ç”¨savepointå¯åŠ¨ï¼‰
        if not savepoint_path:
            logger.warning(f"âš ï¸ æœªæ‰¾åˆ°Savepointï¼Œå°†ä¸ä½¿ç”¨Savepointæ­£å¸¸å¯åŠ¨ä½œä¸š")
            logger.info(f"ä½œä¸šä¿¡æ¯: job_id={job_id}, job_name={original_job_name}")
            
            # å¦‚æœç”¨æˆ·æ˜ç¡®è¯·æ±‚ä½¿ç”¨savepointï¼ˆè¯·æ±‚ä¸­æœ‰è·¯å¾„ä½†æ— æ•ˆï¼‰ï¼Œæ‰æŠ›å‡ºé”™è¯¯
            # å¦åˆ™å…è®¸æ­£å¸¸å¯åŠ¨
            # if req.savepoint_path:  # ç”¨æˆ·æ˜ç¡®æŒ‡å®šäº†ä½†æ²¡æ‰¾åˆ°
            #     raise HTTPException(...)
        
        logger.info(f"ğŸ”„ å¼€å§‹æ¢å¤ä½œä¸š: {original_job_name}")
        logger.info(f"ğŸ“ Savepointè·¯å¾„: {savepoint_path}")
        
        # æ£€æŸ¥SQLæ˜¯å¦åŒ…å«INSERTè¯­å¥
        if not sql_text or 'INSERT' not in sql_text.upper():
            logger.error(f"âŒ SQLä¸ºç©ºæˆ–æ²¡æœ‰INSERTè¯­å¥ï¼Œæ— æ³•æ¢å¤ä½œä¸š")
            raise HTTPException(
                status_code=400,
                detail={
                    "error": "Invalid SQL",
                    "message": "ä½œä¸šSQLä¸­æ²¡æœ‰INSERTè¯­å¥ï¼Œæ— æ³•æ¢å¤",
                    "hint": "è¯·æ£€æŸ¥ä½œä¸šæ˜¯å¦é€šè¿‡å¯è§†åŒ–é…ç½®æ­£ç¡®æäº¤"
                }
            )
        
        logger.info(f"SQLå†…å®¹å‰200å­—ç¬¦: {sql_text[:200]}")
        logger.info(f"æ˜¯å¦åŒ…å«INSERT: {'INSERT' in sql_text.upper()}")
        
        # ä½¿ç”¨SQL Gatewayé‡æ–°æäº¤ä½œä¸šï¼ŒæŒ‡å®šsavepointè·¯å¾„å’ŒåŸå§‹ä½œä¸šåç§°
        logger.info(f"Calling submit_sql_job...")
        result = await flink_client.submit_sql_job(
            sql=sql_text,
            job_name=original_job_name,
            parallelism=parallelism,
            savepoint_path=savepoint_path
        )
        
        logger.info(f"submit_sql_job returned: {result}")
        logger.info(f"Result keys: {list(result.keys())}")
        
        # æ³¨æ„ï¼šflink_clientè¿”å›çš„é”®åæ˜¯"operationHandle"ï¼ˆå¤§å†™Hï¼‰
        job_id_new = result.get("jobid")
        operation_handle = result.get("operationHandle")
        
        logger.info(f"Extracted jobid: {job_id_new}")
        logger.info(f"Extracted operationHandle: {operation_handle}")
        
        if job_id_new:
            # æˆåŠŸè·å–åˆ°ä½œä¸šID
            logger.info(f"Job resumed successfully, new job ID: {job_id_new}")
            
            # è·å–å½“å‰æ—¶é—´ä½œä¸ºå¼€å§‹æ—¶é—´
            start_time = int(time.time() * 1000)

            # ä¿å­˜æ–°ä½œä¸šåˆ°æ•°æ®åº“
            save_job(
                job_id=job_id_new,
                job_name=original_job_name,
                sql_text=sql_text,
                parallelism=parallelism,
                resumed_from_job_id=job_id,
                savepoint_path=req.savepoint_path,
                savepoint_timestamp=savepoint_timestamp,
                start_time=start_time,
                state="RUNNING"
            )
            
            # ä¿å­˜æ–‡ä»¶å¤‡ä»½
            new_sql_file_path = SQL_FILES_DIR / f"{job_id_new}.sql"
            new_sql_file_path.write_text(sql_text, encoding="utf-8")
            
            new_name_file_path = SQL_FILES_DIR / f"{job_id_new}_name.txt"
            new_name_file_path.write_text(original_job_name, encoding="utf-8")
            
            # æ›´æ–°æ—§ä½œä¸šçŠ¶æ€
            update_job_state(job_id, "RESUMED")
            
            # è®°å½•æ“ä½œæ—¥å¿—
            log_operation(
                job_id=job_id_new,
                operation_type="RESTART",
                operation_details={
                    "old_job_id": job_id,
                    "savepoint_path": savepoint_path
                }
            )
            
            return {
                "job_id": job_id_new,
                "old_job_id": job_id,
                "savepoint_path": req.savepoint_path,
                "savepoint_timestamp": req.timestamp,
                "status": "RUNNING",
                "message": "Job resumed from savepoint"
            }
        elif operation_handle:
            # ä½œä¸šæ­£åœ¨å¯åŠ¨ä¸­ï¼Œè¿”å›202çŠ¶æ€ç 
            logger.info(f"Job is starting, operation_handle: {operation_handle}")
            raise HTTPException(
                status_code=202,
                detail={
                    "error": "Job is starting",
                    "message": "Job submitted but still starting, please refresh job list later",
                    "operation_handle": operation_handle,
                    "hint": "Job will appear in job list in a few seconds"
                }
            )
        else:
            # æ—¢æ²¡æœ‰jobidä¹Ÿæ²¡æœ‰operation_handle
            logger.error(f"Unexpected result structure: {result}")
            raise HTTPException(
                status_code=500,
                detail={
                    "error": "Failed to resume job",
                    "message": "Could not get new job ID or operation handle",
                    "debug_info": result
                }
            )
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"æ¢å¤ä½œä¸šå¤±è´¥: {e}")
        import traceback
        logger.error(traceback.format_exc())
        raise HTTPException(
            status_code=500,
            detail={
                "error": "æ¢å¤ä½œä¸šå¤±è´¥",
                "message": str(e)
            }
        )


@app.post("/api/jobs/{job_id}/savepoint", tags=["ä½œä¸šç®¡ç†"])
async def trigger_savepoint(job_id: str, req: SavepointRequest):
    """è§¦å‘ Savepoint"""
    try:
        result = await flink_client.trigger_savepoint(job_id, req.target_directory, req.cancel_job or False)
        return result
    except Exception as e:
        error_msg = str(e)
        if "state.savepoints.dir is not set" in error_msg:
            raise HTTPException(
                status_code=400,
                detail={
                    "error": "Savepoint ç›®å½•æœªé…ç½®",
                    "message": "Flink é›†ç¾¤æœªé…ç½®é»˜è®¤ Savepoint ç›®å½•ï¼Œè¯·å¡«å†™ Savepoint è·¯å¾„",
                    "hint": "åœ¨ flink-conf.yaml ä¸­é…ç½® state.savepoints.dir æˆ–åœ¨æ­¤å¤„å¡«å†™å…·ä½“çš„è·¯å¾„"
                }
            )
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/jobs/{job_id}/savepoints", tags=["ä½œä¸šç®¡ç†"])
async def get_job_savepoints(job_id: str):
    """è·å–ä½œä¸š Savepoints åˆ—è¡¨"""
    try:
        # ä¼˜å…ˆä»æ•°æ®åº“è¯»å–Savepointåˆ—è¡¨
        db_savepoints = get_savepoints(job_id=job_id)
        
        if db_savepoints:
            logger.info(f"âœ… ä»æ•°æ®åº“è¯»å–åˆ° {len(db_savepoints)} ä¸ªSavepoint")
            return {"savepoints": db_savepoints}
        
        # é™çº§åˆ°ä»æ–‡ä»¶è¯»å–savepointè·¯å¾„
        savepoint_file = SQL_FILES_DIR / f"{job_id}_savepoint.txt"
        if savepoint_file.exists():
            savepoint_path = savepoint_file.read_text(encoding="utf-8").strip()
            if savepoint_path:
                logger.info(f"âœ… ä»æ–‡ä»¶è¯»å–åˆ°savepointè·¯å¾„: {savepoint_path}")
                # è·å–å½“å‰æ—¶é—´æˆ³
                import time
                timestamp = int(time.time() * 1000)
                return {
                    "savepoints": [
                        {
                            "id": "saved",
                            "path": savepoint_path,
                            "timestamp": timestamp,
                            "status": "COMPLETED"
                        }
                    ]
                }
        
        # æœ€åå°è¯•ä»Flink APIè·å–
        return await flink_client.get_job_savepoints(job_id)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
        raise HTTPException(status_code=500, detail=str(e))


# ============ ä½œä¸šç›‘æ§ ============
@app.get("/api/jobs/{job_id}/checkpoints", tags=["ä½œä¸šç®¡ç†"])
async def get_job_checkpoints(job_id: str):
    """è·å–ä½œä¸šCheckpointsä¿¡æ¯"""
    try:
        return await flink_client.get_job_checkpoints(job_id)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/jobs/{job_id}/savepoints", tags=["ä½œä¸šç®¡ç†"])
async def get_job_savepoints(job_id: str):
    """è·å–ä½œä¸šä¿å­˜çš„Savepointsä¿¡æ¯ï¼ˆä»æ•°æ®åº“ï¼‰"""
    try:
        from .db_operations import get_savepoints
        return get_savepoints(job_id)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/jobs/{job_id}/metrics", tags=["ä½œä¸šç›‘æ§"])
async def get_job_metrics(job_id: str, metrics: Optional[str] = Query(None, description="æŒ‡æ ‡åç§°ï¼Œé€—å·åˆ†éš”")):
    """è·å–ä½œä¸šæŒ‡æ ‡"""
    try:
        metric_list = metrics.split(",") if metrics else None
        return await flink_client.get_job_metrics(job_id, metric_list)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/jobs/{job_id}/vertices/{vertex_id}/metrics", tags=["ä½œä¸šç›‘æ§"])
async def get_vertex_metrics(
    job_id: str,
    vertex_id: str,
    metrics: Optional[str] = Query(None, description="æŒ‡æ ‡åç§°ï¼Œé€—å·åˆ†éš”")
):
    """è·å–ç®—å­æŒ‡æ ‡"""
    try:
        metric_list = metrics.split(",") if metrics else None
        return await flink_client.get_vertex_metrics(job_id, vertex_id, metric_list)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


# ============ Jar ç®¡ç† ============
@app.get("/api/jars", tags=["Jarç®¡ç†"])
async def get_jars():
    """è·å–å·²ä¸Šä¼ çš„ Jar åˆ—è¡¨"""
    try:
        return await flink_client.get_jars()
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/jars/upload", tags=["Jarç®¡ç†"])
async def upload_jar(file: UploadFile = File(...)):
    """ä¸Šä¼  Jar æ–‡ä»¶"""
    try:
        content = await file.read()
        result = await flink_client.upload_jar(content, file.filename)
        return result
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.delete("/api/jars/{jar_id}", tags=["Jarç®¡ç†"])
async def delete_jar(jar_id: str):
    """åˆ é™¤ Jar"""
    try:
        await flink_client.delete_jar(jar_id)
        return {"status": "deleted", "jar_id": jar_id}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/jars/{jar_id}/run", tags=["Jarç®¡ç†"])
async def run_jar(jar_id: str, req: JarJobSubmitRequest):
    """è¿è¡Œ Jar ä½œä¸š"""
    try:
        result = await flink_client.run_jar(
            jar_id=jar_id,
            entry_class=req.entry_class,
            program_args=req.program_args,
            parallelism=req.parallelism,
            savepoint_path=req.savepoint_path
        )
        job_id = result.get("jobid") or result.get("jobId")
        return JobSubmitResponse(job_name=req.job_name, flink_job_id=job_id)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/jars/{jar_id}/plan", tags=["Jarç®¡ç†"])
async def get_jar_plan(
    jar_id: str,
    entry_class: Optional[str] = None,
    program_args: Optional[str] = None
):
    """è·å– Jar æ‰§è¡Œè®¡åˆ’ï¼ˆä¸å®é™…è¿è¡Œï¼‰"""
    try:
        return await flink_client.get_jar_plan(jar_id, entry_class, program_args)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


# ============ SQL ä½œä¸šï¼ˆæ”¯æŒ SQL Gateway å’Œ SQL Runner Jarï¼‰============
@app.post("/api/sql/submit", tags=["SQLä½œä¸š"])
async def submit_sql_job(req: SqlJobSubmitRequest):
    """
    æäº¤ SQL ä½œä¸š
    ä¼˜å…ˆä½¿ç”¨ SQL Gatewayï¼Œå¦‚æœä¸æ”¯æŒåˆ™å›é€€åˆ° SQL Runner Jar
    ä½œä¸šæäº¤åç«‹å³å†™å…¥æ•°æ®åº“ï¼ŒçŠ¶æ€ä¸º RUNNING
    """
    from .config import SQL_RUNNER_JAR_ID, SQL_GATEWAY_URL
    import traceback
    import time

    logger.info(f"=== æäº¤ SQL ä½œä¸š ===")
    logger.info(f"ä½œä¸šåç§°: {req.job_name}")
    logger.info(f"å¹¶è¡Œåº¦: {req.parallelism}")
    logger.info(f"SQL å†…å®¹ï¼ˆå‰ 200 å­—ç¬¦ï¼‰: {req.sql_text[:200]}")
    logger.info(f"SQL é•¿åº¦: {len(req.sql_text)}")

    # 1. å°è¯•ä½¿ç”¨ SQL Gateway æäº¤
    try:
        logger.info(f"å°è¯•ä½¿ç”¨ SQL Gateway æäº¤ä½œä¸š")
        logger.info(f"SQL Gateway URL: {SQL_GATEWAY_URL}")

        result = await flink_client.submit_sql_job(
            sql=req.sql_text,
            job_name=req.job_name,
            parallelism=req.parallelism
        )

        logger.info(f"SQL Gateway è¿”å›ç»“æœ: {result}")

        if result.get("jobid"):
            # ç«‹å³ä¿å­˜ä½œä¸šä¿¡æ¯åˆ°æ•°æ®åº“
            job_id = result["jobid"]
            logger.info(f"âœ… è·å–åˆ°ä½œä¸š ID: {job_id}")
            
            # ä¿å­˜SQLæ–‡æœ¬åˆ°æ–‡ä»¶ï¼Œä»¥ä¾¿åç»­æ¢å¤
            sql_file_path = SQL_FILES_DIR / f"{job_id}.sql"
            sql_file_path.write_text(req.sql_text, encoding="utf-8")
            logger.info(f"âœ… SQL å·²ä¿å­˜åˆ°: {sql_file_path}")
            
            # ä¿å­˜ç”¨æˆ·é…ç½®çš„ä½œä¸šåç§°
            name_file_path = SQL_FILES_DIR / f"{job_id}_name.txt"
            name_file_path.write_text(req.job_name, encoding="utf-8")
            logger.info(f"âœ… ä½œä¸šåç§°å·²ä¿å­˜åˆ°: {name_file_path}")
            
            # ç«‹å³ä¿å­˜ä½œä¸šä¿¡æ¯åˆ°æ•°æ®åº“
            save_success = save_job(
                job_id=job_id,
                job_name=req.job_name,
                sql_text=req.sql_text,
                parallelism=req.parallelism
            )
            logger.info(f"âœ… ä½œä¸šä¿¡æ¯å·²ä¿å­˜åˆ°æ•°æ®åº“: {job_id}")
            
            # è·å–ä½œä¸šè¯¦æƒ…ï¼Œè®¾ç½®æ—¶é—´ä¿¡æ¯å’ŒFlinkåŸå§‹åç§°
            try:
                job_detail = await flink_client.get_job_detail(job_id)
                start_time = job_detail.get("start-time", int(time.time() * 1000))
                duration = job_detail.get("duration", 0)
                flink_job_name = job_detail.get("name", job_id)  # FlinkåŸå§‹ä½œä¸šåç§°
                logger.info(f"Flinkä½œä¸šè¯¦æƒ…: start_time={start_time}, duration={duration}, flink_name={flink_job_name}")
                
                # æ›´æ–°ä½œä¸šçŠ¶æ€ä¸ºRUNNINGï¼Œå¹¶è®¾ç½®æ—¶é—´ä¿¡æ¯å’ŒFlinkåŸå§‹åç§°
                update_job_state(job_id, "RUNNING", start_time=start_time, duration=duration, flink_job_name=flink_job_name)
                logger.info(f"âœ… ä½œä¸šçŠ¶æ€å·²æ›´æ–°ä¸ºRUNNING: {job_id}")
            except Exception as e:
                logger.warning(f"è·å–ä½œä¸šè¯¦æƒ…å¤±è´¥: {e}")
                # ä½¿ç”¨å½“å‰æ—¶é—´ä½œä¸ºå¼€å§‹æ—¶é—´
                start_time = int(time.time() * 1000)
                update_job_state(job_id, "RUNNING", start_time=start_time, duration=0)
            
            # è®°å½•æ“ä½œæ—¥å¿—
            log_operation(
                job_id=job_id,
                operation_type="SUBMIT",
                operation_details={
                    "job_name": req.job_name,
                    "parallelism": req.parallelism
                }
            )
            
            logger.info(f"âœ… SQL ä½œä¸šæäº¤æˆåŠŸï¼Œä½œä¸š ID: {job_id}")
            return JobSubmitResponse(
                job_name=req.job_name,
                flink_job_id=job_id
            )
        elif result.get("operationHandle"):
            # INSERT è¯­å¥å·²æäº¤ï¼Œä½œä¸šæ­£åœ¨å¯åŠ¨ä¸­ï¼Œç­‰å¾…è·å–çœŸæ­£çš„ job_id
            operation_handle = result.get("operationHandle")
            logger.info(f"âœ… INSERT è¯­å¥å·²æäº¤ï¼Œç­‰å¾…è·å– job_id: {operation_handle}")
            
            # ç«‹å³ä¿å­˜ä¸´æ—¶ä½œä¸šè®°å½•ï¼Œç¡®ä¿"ä¸€æ—¦æäº¤å°±å†™å…¥è¡¨"
            temp_job_id = f"temp_{operation_handle[:32]}"
            
            # ä¿å­˜SQLæ–‡æœ¬åˆ°æ–‡ä»¶
            sql_file_path = SQL_FILES_DIR / f"{temp_job_id}.sql"
            sql_file_path.write_text(req.sql_text, encoding="utf-8")
            
            # ä¿å­˜ç”¨æˆ·é…ç½®çš„ä½œä¸šåç§°
            name_file_path = SQL_FILES_DIR / f"{temp_job_id}_name.txt"
            name_file_path.write_text(req.job_name, encoding="utf-8")
            
            # ä¿å­˜ä½œä¸šä¿¡æ¯åˆ°æ•°æ®åº“ï¼ˆçŠ¶æ€ä¸º INITIALIZINGï¼‰
            save_job(
                job_id=temp_job_id,
                job_name=req.job_name,
                sql_text=req.sql_text,
                parallelism=req.parallelism,
                state="INITIALIZING"
            )
            logger.info(f"âœ… ä¸´æ—¶ä½œä¸šä¿¡æ¯å·²ä¿å­˜åˆ°æ•°æ®åº“: {temp_job_id}")

            import asyncio
            job_id_new = None
            
            # ç­‰å¾…å¹¶å°è¯•è·å– job_id
            for i in range(5):  # æœ€å¤šç­‰å¾… 5 ç§’
                await asyncio.sleep(1)
                try:
                    # ä» Flink REST API è·å–ä½œä¸šåˆ—è¡¨
                    jobs_overview = await flink_client.get_jobs_overview()
                    flink_jobs = jobs_overview.get("jobs", [])
                    
                    # æŸ¥æ‰¾ RUNNING çŠ¶æ€çš„æ–°ä½œä¸š
                    for job in flink_jobs:
                        if job.get("status") == "RUNNING":
                            potential_job_id = job.get("id")
                            # æ£€æŸ¥è¿™ä¸ª job_id æ˜¯å¦å·²ç»åœ¨æ•°æ®åº“ä¸­
                            existing_job = get_job(potential_job_id)
                            if not existing_job:
                                job_id_new = potential_job_id
                                logger.info(f"æ‰¾åˆ°æ–°å¯åŠ¨çš„ä½œä¸š: {job_id_new}")
                                break
                    
                    if job_id_new:
                        break
                except Exception as e:
                    logger.warning(f"ç­‰å¾…è·å– job_id å¤±è´¥: {e}")
            
            if job_id_new:
                # æ‰¾åˆ°çœŸæ­£çš„ job_idï¼Œåˆ é™¤ä¸´æ—¶è®°å½•å¹¶ä¿å­˜æ­£å¼è®°å½•
                logger.info(f"âœ… è·å–åˆ°çœŸæ­£çš„ job_id: {job_id_new}ï¼Œæ›¿æ¢ä¸´æ—¶è®°å½•")
                
                try:
                    from .db_operations import delete_job
                    delete_job(temp_job_id)
                    logger.info(f"å·²åˆ é™¤ä¸´æ—¶è®°å½•: {temp_job_id}")
                except Exception as del_err:
                    logger.warning(f"åˆ é™¤ä¸´æ—¶è®°å½•å¤±è´¥: {del_err}")

                # ä¿å­˜SQLæ–‡æœ¬åˆ°æ–‡ä»¶
                sql_file_path = SQL_FILES_DIR / f"{job_id_new}.sql"
                sql_file_path.write_text(req.sql_text, encoding="utf-8")
                logger.info(f"âœ… SQL å·²ä¿å­˜åˆ°: {sql_file_path}")
                
                # ä¿å­˜ç”¨æˆ·é…ç½®çš„ä½œä¸šåç§°
                name_file_path = SQL_FILES_DIR / f"{job_id_new}_name.txt"
                name_file_path.write_text(req.job_name, encoding="utf-8")
                logger.info(f"âœ… ä½œä¸šåç§°å·²ä¿å­˜åˆ°: {name_file_path}")
                
                # ä¿å­˜ä½œä¸šä¿¡æ¯åˆ°æ•°æ®åº“
                save_job(
                    job_id=job_id_new,
                    job_name=req.job_name,
                    sql_text=req.sql_text,
                    parallelism=req.parallelism
                )
                logger.info(f"âœ… ä½œä¸šä¿¡æ¯å·²ä¿å­˜åˆ°æ•°æ®åº“: {job_id_new} - {req.job_name}")
                
                # è·å–ä½œä¸šè¯¦æƒ…ï¼Œè®¾ç½®æ—¶é—´ä¿¡æ¯å’ŒFlinkåŸå§‹åç§°
                try:
                    job_detail = await flink_client.get_job_detail(job_id_new)
                    start_time = job_detail.get("start-time", int(time.time() * 1000))
                    flink_job_name = job_detail.get("name", job_id_new)
                    update_job_state(job_id_new, "RUNNING", start_time=start_time, flink_job_name=flink_job_name)
                    logger.info(f"FlinkåŸå§‹åç§°: {flink_job_name}")
                except Exception as e:
                    logger.warning(f"è·å–ä½œä¸šè¯¦æƒ…å¤±è´¥: {e}")
                    update_job_state(job_id_new, "RUNNING", start_time=int(time.time() * 1000))
                
                # è®°å½•æ“ä½œæ—¥å¿—
                log_operation(
                    job_id=job_id_new,
                    operation_type="SUBMIT",
                    operation_details={
                        "job_name": req.job_name,
                        "parallelism": req.parallelism,
                        "operation_handle": operation_handle
                    }
                )
                
                return JobSubmitResponse(
                    job_name=req.job_name,
                    flink_job_id=job_id_new
                )
            else:
                # æœªèƒ½è·å–åˆ° job_idï¼Œä¿ç•™ä¸´æ—¶ ID
                logger.warning(f"æœªèƒ½è·å–åˆ° job_idï¼Œä½¿ç”¨ä¸´æ—¶ ID: {temp_job_id}")
                
                return {
                    "job_name": req.job_name,
                    "flink_job_id": None,
                    "operation_handle": operation_handle,
                    "temp_job_id": temp_job_id,
                    "message": "ä½œä¸šå·²æäº¤ï¼Œæ­£åœ¨å¯åŠ¨ä¸­",
                    "status": "success"
                }
        elif result.get("operationHandle"):
            logger.info(f"âœ… SQL å·²æˆåŠŸæ‰§è¡Œï¼ˆæŸ¥è¯¢è¯­å¥ï¼‰ï¼Œæ“ä½œå¥æŸ„: {result['operationHandle']}")
            # SQL å·²æäº¤ä½†ä¸æ˜¯ INSERT è¯­å¥
            return {
                "job_name": req.job_name,
                "flink_job_id": None,
                "operation_handle": result.get("operationHandle"),
                "message": "SQL å·²æˆåŠŸæ‰§è¡Œï¼ˆæŸ¥è¯¢è¯­å¥ï¼‰",
                "status": "success"
            }
        elif result.get("status") == "no_insert":
            logger.info(f"âš ï¸ æ²¡æœ‰æ‰¾åˆ° INSERT è¯­å¥")
            return {
                "job_name": req.job_name,
                "flink_job_id": None,
                "message": "SQL å·²æˆåŠŸæ‰§è¡Œï¼Œä½†æ²¡æœ‰ INSERT è¯­å¥",
                "status": "success"
            }
        else:
            # æ²¡æœ‰è¿”å› jobid æˆ– operationHandleï¼Œå°è¯•ä½¿ç”¨ Jar
            logger.warning(f"SQL Gateway å“åº”å¼‚å¸¸ï¼Œå°è¯•ä½¿ç”¨ Jar æ–¹å¼: {result}")
            raise Exception("SQL Gateway å“åº”ä¸­ç¼ºå°‘ä½œä¸š ID")

    except Exception as e:
        error_msg = str(e)
        logger.error(f"âŒ SQL Gateway æäº¤å¤±è´¥: {error_msg}")
        logger.error(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")

        # SQL Gateway ä¸å¯ç”¨ï¼Œå°è¯•ä½¿ç”¨ Jar æ–¹å¼
        if "404" in error_msg or "SQL Gateway" in error_msg or "Not Found" in error_msg or "æ²¡æœ‰è¿”å›ä½œä¸š ID" in error_msg:
            logger.info("SQL Gateway ä¸å¯ç”¨ï¼Œå°è¯•ä½¿ç”¨ SQL Runner Jar æ–¹å¼")

            # 2. ä¿å­˜ SQL æ–‡æœ¬åˆ°æ–‡ä»¶
            file_id = str(uuid.uuid4())
            sql_file_path = SQL_FILES_DIR / f"{file_id}.sql"
            sql_file_path.write_text(req.sql_text, encoding="utf-8")
            logger.info(f"SQL æ–‡ä»¶å·²ä¿å­˜åˆ°: {sql_file_path}")

            # 3. æ£€æŸ¥ SQL Runner Jar é…ç½®
            if not SQL_RUNNER_JAR_ID or SQL_RUNNER_JAR_ID.strip() == "":
                logger.error("âŒ SQL Runner Jar æœªé…ç½®")
                raise HTTPException(
                    status_code=400,
                    detail={
                        "error": "SQL Gateway ä¸å¯ç”¨ä¸” SQL Runner Jar æœªé…ç½®",
                        "sql_gateway_error": str(e),
                        "message": "æ‚¨çš„é›†ç¾¤ä¸æ”¯æŒ SQL Gatewayï¼Œä¸”æœªé…ç½® SQL Runner Jar",
                        "sql_gateway_url": SQL_GATEWAY_URL,
                        "steps": [
                            "æ–¹å¼1ï¼šé…ç½® SQL Runner Jar",
                            "- è®¿é—® http://localhost:8000/api/jars æŸ¥çœ‹å·²ä¸Šä¼ çš„ Jar",
                            "- å¤åˆ¶ Jar ID åˆ° backend/config.py çš„ SQL_RUNNER_JAR_ID",
                            "æ–¹å¼2ï¼šå¯ç”¨ Flink SQL Gateway",
                            "- åœ¨ Flink é…ç½®ä¸­å¯ç”¨ SQL Gateway (flink-sql-gateway)",
                            "æ–¹å¼3ï¼šå¤åˆ¶ SQL åˆ° Flink SQL Client æ‰§è¡Œ"
                        ]
                    }
                )

            logger.info(f"ä½¿ç”¨ SQL Runner Jar æäº¤ä½œä¸š: {SQL_RUNNER_JAR_ID}")

            # 4. ä½¿ç”¨ SQL Runner Jar æäº¤
            try:
                result = await flink_client.run_jar(
                    jar_id=SQL_RUNNER_JAR_ID,
                    program_args=f"--sqlFile {sql_file_path} --jobName {req.job_name}",
                    parallelism=req.parallelism
                )
                job_id = result.get("jobid") or result.get("jobId")
                logger.info(f"âœ… SQL Runner Jar æäº¤æˆåŠŸï¼Œä½œä¸š ID: {job_id}")
                return JobSubmitResponse(job_name=req.job_name, flink_job_id=job_id)
            except Exception as jar_error:
                logger.error(f"âŒ SQL Runner Jar æäº¤å¤±è´¥: {str(jar_error)}")
                logger.error(f"Jar é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
                raise HTTPException(
                    status_code=500,
                    detail=f"SQL Gateway å’Œ SQL Runner Jar éƒ½å¤±è´¥ã€‚Gateway é”™è¯¯: {str(e)}, Jar é”™è¯¯: {str(jar_error)}"
                )
        else:
            logger.error(f"âŒ æäº¤ SQL ä½œä¸šå¤±è´¥: {error_msg}")
            raise HTTPException(
                status_code=500,
                detail=f"æäº¤ SQL ä½œä¸šå¤±è´¥: {error_msg}"
            )


@app.get("/api/sql/history", tags=["SQLä½œä¸š"])
async def get_sql_history():
    """è·å–å·²ä¿å­˜çš„ SQL æ–‡ä»¶åˆ—è¡¨"""
    sql_files = []
    for f in SQL_FILES_DIR.glob("*.sql"):
        content = f.read_text(encoding="utf-8")
        sql_files.append({
            "id": f.stem,
            "filename": f.name,
            "content": content[:500] + "..." if len(content) > 500 else content,
            "size": f.stat().st_size,
            "created": f.stat().st_ctime
        })
    return sorted(sql_files, key=lambda x: x["created"], reverse=True)


@app.get("/api/sql/{file_id}/content", tags=["SQLä½œä¸š"])
async def get_sql_content(file_id: str):
    """è·å–æŒ‡å®š SQL æ–‡ä»¶çš„å®Œæ•´å†…å®¹"""
    try:
        sql_file_path = SQL_FILES_DIR / f"{file_id}.sql"
        if not sql_file_path.exists():
            raise HTTPException(status_code=404, detail="SQLæ–‡ä»¶ä¸å­˜åœ¨")
        
        content = sql_file_path.read_text(encoding="utf-8")
        return {
            "id": file_id,
            "filename": f"{file_id}.sql",
            "content": content
        }
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


# ============ æ•°æ®æºç®¡ç† ============

@app.post("/api/datasources", tags=["æ•°æ®æºç®¡ç†"])
async def create_datasource(request: DataSourceCreateRequest):
    """åˆ›å»ºæ•°æ®æº"""
    try:
        success = add_datasource(
            name=request.name,
            type=request.type,
            host=request.host,
            port=request.port,
            username=request.username,
            password=request.password,
            database=request.database,
            properties=request.properties
        )
        if success:
            return {"status": "success", "message": "æ•°æ®æºå·²åˆ›å»º"}
        else:
            raise HTTPException(status_code=500, detail="åˆ›å»ºæ•°æ®æºå¤±è´¥")
    except Exception as e:
        logger.error(f"åˆ›å»ºæ•°æ®æºå¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/datasources", tags=["æ•°æ®æºç®¡ç†"], response_model=List[DataSourceResponse])
async def list_datasources():
    """è·å–æ•°æ®æºåˆ—è¡¨"""
    try:
        return get_datasources()
    except Exception as e:
        logger.error(f"è·å–æ•°æ®æºåˆ—è¡¨å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.put("/api/datasources/{id}", tags=["æ•°æ®æºç®¡ç†"])
async def update_datasource_endpoint(id: int, request: DataSourceUpdateRequest):
    """æ›´æ–°æ•°æ®æº"""
    try:
        success = update_datasource(
            id=id,
            name=request.name,
            type=request.type,
            host=request.host,
            port=request.port,
            username=request.username,
            password=request.password,
            database=request.database,
            properties=request.properties
        )
        if success:
            return {"status": "success", "message": "æ•°æ®æºå·²æ›´æ–°"}
        else:
            raise HTTPException(status_code=404, detail="æ•°æ®æºä¸å­˜åœ¨")
    except Exception as e:
        logger.error(f"æ›´æ–°æ•°æ®æºå¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.delete("/api/datasources/{id}", tags=["æ•°æ®æºç®¡ç†"])
async def delete_datasource_endpoint(id: int):
    """åˆ é™¤æ•°æ®æº"""
    try:
        success = delete_datasource(id)
        if success:
            return {"status": "success", "message": "æ•°æ®æºå·²åˆ é™¤"}
        else:
            raise HTTPException(status_code=404, detail="æ•°æ®æºä¸å­˜åœ¨")
    except Exception as e:
        logger.error(f"åˆ é™¤æ•°æ®æºå¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/datasources/test", tags=["æ•°æ®æºç®¡ç†"])
async def test_datasource(request: DataSourceBase):
    """æµ‹è¯•æ•°æ®æºè¿æ¥"""
    try:
        success, message = test_datasource_connection(
            type=request.type,
            host=request.host,
            port=request.port,
            username=request.username,
            password=request.password,
            database=request.database
        )
        if success:
            return {"status": "success", "message": message}
        else:
            return {"status": "error", "message": message}
    except Exception as e:
        logger.error(f"æµ‹è¯•æ•°æ®æºè¿æ¥å¤±è´¥: {e}")
        return {"status": "error", "message": str(e)}

@app.get("/api/datasources/{id}/tables", tags=["æ•°æ®æºç®¡ç†"])
async def get_datasource_tables_api(id: int):
    """è·å–æ•°æ®æºä¸‹çš„æ‰€æœ‰è¡¨"""
    try:
        from .db_operations import get_datasource_tables
        tables = get_datasource_tables(id)
        return {"tables": tables}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/datasources/{id}/tables/{table_name}/columns", tags=["æ•°æ®æºç®¡ç†"])
async def get_datasource_columns_api(id: int, table_name: str):
    """è·å–æ•°æ®æºä¸‹æŒ‡å®šè¡¨çš„å­—æ®µ"""
    try:
        from .db_operations import get_datasource_columns
        columns = get_datasource_columns(id, table_name)
        return {"columns": columns}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


# ============ é™æ€æ–‡ä»¶æœåŠ¡ï¼ˆå‰ç«¯é¡µé¢ï¼‰============
FRONTEND_DIR = BASE_DIR.parent / "frontend" / "dist"

# å¦‚æœå‰ç«¯å·²æ„å»ºï¼ŒæŒ‚è½½é™æ€æ–‡ä»¶
if FRONTEND_DIR.exists():
    app.mount("/assets", StaticFiles(directory=FRONTEND_DIR / "assets"), name="assets")

    @app.get("/", tags=["å‰ç«¯é¡µé¢"])
    async def serve_frontend():
        """è¿”å›å‰ç«¯é¦–é¡µ"""
        return FileResponse(FRONTEND_DIR / "index.html")

    @app.get("/{full_path:path}", tags=["å‰ç«¯é¡µé¢"])
    async def serve_frontend_routes(full_path: str):
        """å¤„ç†å‰ç«¯è·¯ç”±"""
        # å¦‚æœæ˜¯ API è·¯å¾„ï¼Œè·³è¿‡
        if full_path.startswith("api/") or full_path.startswith("docs") or full_path.startswith("openapi"):
            raise HTTPException(status_code=404)
        # å°è¯•è¿”å›é™æ€æ–‡ä»¶
        file_path = FRONTEND_DIR / full_path
        if file_path.exists() and file_path.is_file():
            return FileResponse(file_path)
        # å¦åˆ™è¿”å› index.htmlï¼ˆSPA è·¯ç”±ï¼‰
        return FileResponse(FRONTEND_DIR / "index.html")


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
